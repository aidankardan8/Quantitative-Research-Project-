{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2872,
   "id": "7c42edd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and Import Necessary Libraries and Packages \n",
    "\n",
    "import pandas as pd \n",
    "import pandas_datareader as pdr\n",
    "import pandas_datareader.data as web\n",
    "import numpy as np \n",
    "import yfinance as yf \n",
    "import matplotlib.pyplot as plt \n",
    "import ta as ta \n",
    "import requests \n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy import stats\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import pandas2ri\n",
    "from rpy2.robjects import conversion\n",
    "from scipy.stats import skew, kurtosis\n",
    "pandas2ri.activate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41886698",
   "metadata": {},
   "source": [
    "Before diving into the core research project, it's valuable to take a comprehensive look at the ta package. This package offers a wide array of built-in technical indicators, which can serve as powerful tools for developing and testing trading strategies. \n",
    "\n",
    "However, understanding the mathematical foundations and the various parameters available within the ta package is crucial to tailoring these indicators effectively to fit specific research objectives. While ta offers a robust set of tools, there will undoubtedly be cases where further customization is required to meet more complex research needs. This is where creating custom technical indicators based on specific financial criteria becomes essential. \n",
    "\n",
    "By combining the built-in tools with personalized adjustments, it becomes possible to address nuanced research questions and trading strategies that go beyond standard \n",
    "implementations. Nevertheless, exploring and experimenting with different strategies using the capabilities of ta is not only feasible but also provides a solid foundation for generating insights and enhancing decision-making. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0998007d",
   "metadata": {},
   "source": [
    "# Quantitative Research Project Plan\n",
    "\n",
    "## Step-by-Step Process\n",
    "\n",
    "### 0: Problem Selection, Data Extraction, Scraping, and Cleaning (Completed)\n",
    "\n",
    "- **Objective**: Obtain comprehensive data for any ticker in the S&P 500 over any date and time range, including OHLCV (Open, High, Low, Close, Volume) data and all indicators from the `ta` package.\n",
    "- **Data Quality**: Ensured consistency in data cleaning, handled missing values, and managed outliers effectively.\n",
    "- **Data Sources**: Used reliable data sources (e.g., Yahoo Finance API) for the necessary data frequency (daily, hourly, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "### 1: Exploratory Data Analysis (EDA) (Completed)\n",
    "\n",
    "- **Statistical Analysis**:\n",
    "  - Computed **correlations**, **skewness**, and **kurtosis** among variables.\n",
    "  - Checked for **multicollinearity** using **Variance Inflation Factor (VIF)**.\n",
    "  \n",
    "- **Visualizations**:\n",
    "  - Generated **scatterplots**, **histograms**, and **heatmaps** to visualize data distributions and relationships.\n",
    "  \n",
    "- **Feature Distributions**:\n",
    "  - Assessed normality of feature distributions and applied transformations (e.g., **Box-Cox**, **log transformations**) as needed.\n",
    "\n",
    "---\n",
    "\n",
    "### 2: Predictor Selection (Completed)\n",
    "\n",
    "- **Feature Selection Techniques**:\n",
    "  - Used **VIF** to eliminate highly collinear predictors.\n",
    "  - Applied **ElasticNet regularization** for automatic variable selection and regularization.\n",
    "\n",
    "---\n",
    "\n",
    "### 3: Time Series Analysis (TSA) with ARIMA (Completed)\n",
    "\n",
    "- **Objective**: Modeled and forecasted the time-dependent structure using ARIMA.\n",
    "- **Stationarity**:\n",
    "  - Tested for stationarity using the **ADF test** and applied differencing.\n",
    "- **Model Estimation**: \n",
    "  - Fitted ARIMA and **ARIMAX** models to capture both autoregressive and moving average components.\n",
    "- **Model Selection**: \n",
    "  - Used **AIC/BIC** to compare models and selected the best-performing ones.\n",
    "\n",
    "---\n",
    "\n",
    "### 4: Advanced TSA: Volatility Modeling with GARCH/EGARCH (Completed)\n",
    "\n",
    "- **Objective**: Modeled volatility clustering and heteroscedasticity using **GARCH** and **EGARCH** models.\n",
    "- **Model Estimation**:\n",
    "  - Fitted **GARCH(1,1)** and **EGARCH(1,1)** models with both **Normal** and **Student-t** distributions.\n",
    "  - **EGARCH(1,1) with Student-t distribution** was selected as the best model based on **AIC/BIC** and residual diagnostics.\n",
    "\n",
    "---\n",
    "\n",
    "### 5: Integration of EGARCH Model into Regression Framework (Completed)\n",
    "\n",
    "- **Objective**: Enhanced robust and regularized regression models by incorporating volatility estimates from the EGARCH model.\n",
    "- **EGARCH-Enhanced Regression Models**: \n",
    "  - Integrated **EGARCH volatility** as an additional predictor in robust methods (Huber, Quantile, LTS) and regularized regression models (Ridge, Lasso, ElasticNet).\n",
    "- **Results**:\n",
    "  - Some models, such as **Ridge** and **LTS**, showed modest improvements, while others, like **ElasticNet** and **Lasso**, worsened after EGARCH integration. \n",
    "\n",
    "---\n",
    "\n",
    "### 6: Regularized Regression Models (Completed)\n",
    "\n",
    "- **Objective**: Predicted the target variable using regularized regression methods.\n",
    "- **Models Fit**: \n",
    "  - **Ridge**, **Lasso**, and **ElasticNet** models were applied.\n",
    "- **Evaluation**: \n",
    "  - **MSPE** was used to evaluate the predictive performance. The **Full Model** performed best among the regularized approaches, although adding EGARCH did not lead to significant improvements.\n",
    "\n",
    "---\n",
    "\n",
    "## Future Methodologies and Additions\n",
    "\n",
    "---\n",
    "\n",
    "### Residual Diagnostics and Error Distribution Analysis (In Progress)\n",
    "\n",
    "- **Objective**: Conduct residual diagnostics and assess model fit.\n",
    "- **Error Assumptions**: \n",
    "  - Compare residual distributions using **Q-Q plots** and perform diagnostic tests like **Ljung-Box** and **ARCH** tests to better understand the behavior of residuals post-EGARCH integration.\n",
    "\n",
    "---\n",
    "\n",
    "### Non-Parametric and Machine Learning Methods (Planned)\n",
    "\n",
    "- **Objective**: Explore non-linear relationships and interactions using **machine learning algorithms**.\n",
    "- **Methods to Apply**:\n",
    "  - **Random Forests** and **Gradient Boosting Machines (GBMs)** to identify potential non-linear patterns that EGARCH and traditional regressions missed.\n",
    "\n",
    "---\n",
    "\n",
    "### Ensemble Methods for Model Combination (Planned)\n",
    "\n",
    "- **Objective**: Combine models to improve overall predictive performance.\n",
    "- **Techniques**:\n",
    "  - **Stacking** and **blending** models, allowing for improved accuracy by combining insights from both traditional regression and machine learning models.\n",
    "\n",
    "---\n",
    "\n",
    "### Model Validation and Backtesting (Planned)\n",
    "\n",
    "- **Objective**: Ensure model robustness through **backtesting** and **cross-validation**.\n",
    "- **Techniques**: \n",
    "  - Use **rolling window validation** and **walk-forward validation** to assess the model’s predictive capacity over time and across different market regimes.\n",
    "  - Evaluate the model’s performance using financial metrics like **Sharpe Ratio** and **Value at Risk (VaR)**.\n",
    "\n",
    "---\n",
    "\n",
    "### Advanced Optimization and Simulation Techniques (Planned)\n",
    "\n",
    "- **Objective**: Enhance parameter estimation and evaluate model uncertainty using advanced methods.\n",
    "- **Optimization Techniques**:\n",
    "  - Implement **Simulated Annealing**, **Genetic Algorithms**, or **Particle Swarm Optimization** to improve model fitting.\n",
    "- **Monte Carlo Simulations**:\n",
    "  - Use simulations to stress-test models under various market conditions and assess their risk profiles.\n",
    "\n",
    "---\n",
    "\n",
    "### Distribution Fitting and Simulation (Planned)\n",
    "\n",
    "- **Objective**: Fit and simulate stock price distributions using advanced techniques.\n",
    "- **Methods**:\n",
    "  - Apply **Inverse CDF**, **Acceptance-Rejection methods**, and **Kernel Density Estimation** for distribution fitting, to better model real-world stock price movements.\n",
    "\n",
    "---\n",
    "\n",
    "## Completed Methodologies\n",
    "\n",
    "- **Data Extraction, Cleaning, and Transformation**\n",
    "- **Exploratory Data Analysis (EDA)**\n",
    "- **Variable Transformations and Selection**\n",
    "- **Time Series Analysis (ARIMA)**\n",
    "- **Advanced TSA with GARCH/EGARCH**\n",
    "- **EGARCH Integration with Robust and Regularized Regression Models**\n",
    "- **Residual Diagnostics and Error Analysis**\n",
    "\n",
    "---\n",
    "\n",
    "## Remaining Objectives\n",
    "\n",
    "- **Non-Parametric Methods** (Random Forests, GBM)\n",
    "- **Model Combination and Ensemble Methods**\n",
    "- **Backtesting Framework and Risk Management Metrics**\n",
    "- **Advanced Optimization Techniques** (Simulated Annealing, MCMC)\n",
    "- **Monte Carlo Simulations for Risk Assessment**\n",
    "- **Distribution Fitting and Simulation Methods**\n",
    "\n",
    "---\n",
    "\n",
    "## Practical Implications and Moving Forward\n",
    "\n",
    "- The results from integrating EGARCH models into regularized and robust regression frameworks did not yield the expected improvements. In particular, **ElasticNet** and **Lasso** models worsened after incorporating volatility predictions, while **Ridge** and **LTS** saw only modest gains. \n",
    "- This implies that while volatility modeling is essential, its straightforward inclusion as a predictor may not be the most effective way to improve regression models' performance.\n",
    "  \n",
    "### In Progress:\n",
    "- I plan to explore **non-parametric methods** like Random Forests and GBM to capture non-linear relationships that EGARCH and linear models may have missed.\n",
    "- **Ensemble methods** will also be tested to combine different models for better predictive accuracy.\n",
    "  \n",
    "The journey so far has been very insightful, revealing that traditional regression and volatility modeling may not always yield the best results in complex financial datasets. However, the lessons learned from this exploration will guide my next steps toward building a more robust quantitative strategy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2797,
   "id": "63ba4fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding ta library more comprehensively \n",
    "# Uncomment code to run \n",
    "# print(dir(ta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2798,
   "id": "21e9d733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine submodules of interest\n",
    "# Uncomment code to run \n",
    "# for submodule in dir(ta):\n",
    "    # if not submodule.startswith(\"__\"):\n",
    "       #  print(submodule)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2799,
   "id": "123753af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Trend-based Technical Indicators and how to customize arguments and parameters\n",
    "# Remove # to learn more \n",
    "# help(ta.trend)\n",
    "\n",
    "# List of Volatility-based Technical Indicators and how to customize arguments and parameters\n",
    "# Remove # to learn more \n",
    "# help(ta.volatility)\n",
    "\n",
    "# List of Momentum Based Technical Indicators and how to customize arguments and parameters \n",
    "# Remove # to learn more \n",
    "# help(ta.momentum)\n",
    "\n",
    "# Extensive list of Volume-based Technical Indicators and how to customize arguments and parameters\n",
    "# Remove # to learn more \n",
    "# help(ta.volume)\n",
    "\n",
    "# Popular Technical Indicators bundled together \n",
    "# Remove # to learn more \n",
    "# help(ta.wrapper)\n",
    "\n",
    "# Remove # to learn more \n",
    "# help(ta.add_others_ta)\n",
    "\n",
    "# Remove # to learn more \n",
    "# help(ta.add_trend_ta)\n",
    "\n",
    "# Remove # to learn more \n",
    "# help(ta.add_volatility_ta)\n",
    "\n",
    "# Remove # to learn more \n",
    "# help(ta.add_volume_ta)\n",
    "\n",
    "# Remove # to learn more \n",
    "# help(ta.others)\n",
    "\n",
    "# Remove # to learn more \n",
    "# help(ta.utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2800,
   "id": "3f70cd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In depth analysis of ta package and submodules is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2801,
   "id": "a79702bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initially accessed list of tickers for S&P 500 stocks using slickcharts\n",
    "# Used headers to simulate a request from a web browser\n",
    "# Create a function that grabs all S&P 500 Tickers \n",
    "# Creating this function in anticipation of using it to extract data easily from yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2802,
   "id": "418dd0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sp_500_tickers():\n",
    "    \n",
    "    # Grab url \n",
    "    url = 'https://www.slickcharts.com/sp500'\n",
    "    # Simulate a request from a web browser\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    #Error Handling \n",
    "    try: \n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()  # Ensure the request was successful\n",
    "    except requests.exceptions.HTTPError as http_err: \n",
    "        print(\"HTTP Error has occurred\")\n",
    "        return []\n",
    "    except Exception as err: \n",
    "        print(\"Error has occurred\")\n",
    "        return []\n",
    "    \n",
    "    # Parse the HTML content of the webpage\n",
    "    html_parser = BeautifulSoup(response.text, 'html.parser')\n",
    "    # Scrape html file using inspect and determine how to extract table \n",
    "    # Table was labeled 'table table-hover table-borderless table-sm'\n",
    "    table = html_parser.find('table', {'class': 'table table-hover table-borderless table-sm'})\n",
    "    \n",
    "    if table is None:\n",
    "        print(\"Could not find the table on the webpage.\")\n",
    "        return []\n",
    "    tickers = []\n",
    "    ticker_column_index = 2  # The index of the column containing the ticker symbol\n",
    "    \n",
    "    # Go through all table rows \n",
    "    for row in table.find_all('tr')[1:]:  # Skip the header row\n",
    "        cells = row.find_all('td') # Extract the columns of each row\n",
    "        if len(cells) > ticker_column_index:\n",
    "            ticker = cells[ticker_column_index].text.strip()\n",
    "            tickers.append(ticker)\n",
    "    \n",
    "    return tickers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14057290",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = sp_500_tickers()\n",
    "print(f\"Number of tickers fetched: {len(tickers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2804,
   "id": "c433e379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove # to learn more\n",
    "# help(yf.Ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2805,
   "id": "e8f531f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_tickers(tickers): # Function to validate the tickers by fetching minimal data using yfinance\n",
    "    valid_tickers = []\n",
    "\n",
    "    for ticker in tickers:\n",
    "# Ran into problem here\n",
    "# Determined the root cause was the syntax carried over from Wikipedia was not compatible with yfinance \n",
    "# Made sure that data has been read in wikipedia and yfinance together and data is not empty for any ticker \n",
    "        yf_ticker = ticker.replace('.', '-')\n",
    "        stock = yf.Ticker(yf_ticker)\n",
    "        hist = stock.history(period=\"1m\")\n",
    "        if not hist.empty:\n",
    "            valid_tickers.append(yf_ticker)  # Append the ticker, not the data\n",
    "    \n",
    "    return valid_tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ca16b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the tickers\n",
    "valid_tickers = validate_tickers(tickers)\n",
    "print(f\"Number of valid tickers: {len(valid_tickers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2807,
   "id": "f4fba61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_historical_data(valid_tickers, start_date='2009-01-01', end_date='2024-01-01'): # Function to fetch historical data for validated tickers using yfinance\n",
    "    all_data = {}  # Initialize all_data dictionary\n",
    "\n",
    "    for ticker in valid_tickers:\n",
    "        stock = yf.Ticker(ticker)\n",
    "        \n",
    "        try:\n",
    "            # Fetch historical market data for the specified period\n",
    "            hist = stock.history(start=start_date, end=end_date)\n",
    "            if not hist.empty:\n",
    "                all_data[ticker] = hist\n",
    "            else:\n",
    "                print(f\"No data found for {ticker}.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching historical data for {ticker}: {e}\")\n",
    "    \n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b72dc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch full historical data for the validated tickers\n",
    "historical_data = fetch_historical_data(valid_tickers)\n",
    "print(f\"Number of tickers with historical data: {len(historical_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4047955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select several stocks for visualization to make sure data has been extracted as desired\n",
    "\n",
    "selected_tickers = ['AAPL', 'MSFT', 'GOOGL']  # Selected stock tickers for visualization\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "for ticker in selected_tickers:\n",
    "    if ticker in historical_data:\n",
    "        plt.plot(historical_data[ticker].index, historical_data[ticker]['Close'], label=ticker)\n",
    "\n",
    "plt.title('Historical Close Prices of Selected Stocks from Slick')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204be734",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sp_500_tickers_wiki(): # Function to scrape S&P 500 tickers from Wikipedia \n",
    "    url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies' # Extracted the same data from Wikipedia to ensure consistency and accuracy from external websites\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    # Parse the HTML content of the webpage\n",
    "    html_parser = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Scrape HTML file using inspect and determine how to extract table \n",
    "    table = html_parser.find('table', {'id': 'constituents'})\n",
    "    \n",
    "    # Verify if the table was found\n",
    "    if table is None:\n",
    "        print(\"Could not find the table on the webpage.\")\n",
    "        return []\n",
    "    \n",
    "    # List to store the extracted ticker symbols\n",
    "    tickers = []\n",
    "    ticker_column_index = 0  # The index of the column containing the ticker symbol\n",
    "    \n",
    "    # Go through all table rows\n",
    "    for row in table.find_all('tr')[1:]:  # Skip the header row\n",
    "        cells = row.find_all('td')  # Extract the columns of each row \n",
    "        if len(cells) > ticker_column_index:\n",
    "            ticker = cells[ticker_column_index].text.strip()\n",
    "            tickers.append(ticker)\n",
    "    \n",
    "    return tickers \n",
    "\n",
    "# Example usage, and testing to make sure the process works \n",
    "tickers_wiki = sp_500_tickers_wiki()\n",
    "\n",
    "print(f\"Number of tickers fetched: {len(tickers)}\")\n",
    "\n",
    "# Function to validate the tickers by fetching minimal data using yfinance\n",
    "def validate_tickers_wiki(tickers_wiki):\n",
    "    valid_tickers = []\n",
    "\n",
    "    for ticker in tickers_wiki:\n",
    "        yf_ticker = ticker.replace('.', '-')\n",
    "        stock = yf.Ticker(yf_ticker)\n",
    "        hist = stock.history(period=\"1m\")\n",
    "        if not hist.empty:\n",
    "            valid_tickers.append(yf_ticker)  # Append the ticker, not the data\n",
    "    \n",
    "    return valid_tickers\n",
    "\n",
    "# Validate the tickers\n",
    "valid_tickers_wiki = validate_tickers_wiki(tickers_wiki)\n",
    "print(f\"Number of valid tickers: {len(valid_tickers_wiki)}\")\n",
    "\n",
    "# Function to fetch historical data for validated tickers using yfinance\n",
    "def fetch_historical_data_wiki(valid_tickers_wiki, start_date='2009-01-01', end_date='2024-01-01'):\n",
    "    all_data = {}  # Initialize all_data dictionary\n",
    "\n",
    "    for ticker in valid_tickers_wiki:\n",
    "        stock = yf.Ticker(ticker)\n",
    "        \n",
    "        try:\n",
    "            # Fetch historical market data for the specified period\n",
    "            hist = stock.history(start=start_date, end=end_date)\n",
    "            if not hist.empty:\n",
    "                all_data[ticker] = hist\n",
    "            else:\n",
    "                print(f\"No data found for {ticker}.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching historical data for {ticker}: {e}\")\n",
    "    \n",
    "    return all_data\n",
    "\n",
    "# Fetch full historical data for the validated tickers\n",
    "historical_data_wiki = fetch_historical_data_wiki(valid_tickers_wiki)\n",
    "print(f\"Number of tickers with historical data: {len(historical_data_wiki)}\")\n",
    "\n",
    "# Select several stocks for visualization to make sure data has been extracted as desired\n",
    "selected_tickers_wiki = ['AAPL', 'MSFT', 'GOOGL']  # Selected stock tickers for visualization\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "for ticker in selected_tickers_wiki:\n",
    "    if ticker in historical_data_wiki:\n",
    "        plt.plot(historical_data_wiki[ticker].index, historical_data_wiki[ticker]['Close'], label=ticker)\n",
    "\n",
    "plt.title('Historical Close Prices of Selected Stocks from Wiki')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2811,
   "id": "5b3ddafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_all_indicators(data):\n",
    "    # Ensure necessary columns are present\n",
    "    if 'Open' in data.columns and 'High' in data.columns and 'Low' in data.columns and 'Close' in data.columns and 'Volume' in data.columns:\n",
    "        # Add all technical indicators using ta\n",
    "        data = ta.add_all_ta_features(\n",
    "            data, open=\"Open\", high=\"High\", low=\"Low\", close=\"Close\", volume=\"Volume\", fillna=True)\n",
    "    else:\n",
    "        print(\"Required columns are missing from the data.\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2812,
   "id": "db90b26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_enhance_data(tickers, start_date, end_date):\n",
    "    # Initialize dictionary to store the historical data\n",
    "    all_data = {}\n",
    "    \n",
    "    # Fetch data for each ticker\n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            stock = yf.Ticker(ticker)\n",
    "            historical_data = stock.history(start=start_date, end=end_date)\n",
    "            if not historical_data.empty:\n",
    "                # Apply all technical indicators\n",
    "                all_data[ticker] = apply_all_indicators(historical_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {ticker}: {e}\")\n",
    "    \n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ab7289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your tickers and time periods\n",
    "tickers = ['AAPL', 'MSFT', 'GOOGL']  # Example subset for testing\n",
    "start_date = '2010-01-01'\n",
    "end_date = '2023-12-29'\n",
    "\n",
    "# Fetch and enhance data\n",
    "enhanced_data = fetch_and_enhance_data(tickers, start_date, end_date)\n",
    "\n",
    "# Looking at documentation, others_ta indicator others_cr is calculated only using Close price, our response, and results in a perfect correlation between others_cr and Close \n",
    "# This could potentially ruin the model because of perfect multicollinearity. \n",
    "enhanced_data['AAPL'] = enhanced_data['AAPL'].drop(columns=['others_cr'])\n",
    "# Now you can inspect or save the enhanced data\n",
    "if 'AAPL' in enhanced_data:\n",
    "    print(enhanced_data['AAPL'].head())  # Display some of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cee744",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(enhanced_data['AAPL'].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf87995c",
   "metadata": {},
   "source": [
    "#### Send Data to R:\n",
    "\n",
    "ro.globalenv['variable_name_in_r'] = pandas2ri.py2rpy(pandas_df_in_python)\n",
    "\n",
    "### Run R Code:\n",
    "\n",
    "ro.r('''\n",
    "    R code here\n",
    "''')\n",
    "\n",
    "### Retrieve Data from R:\n",
    "\n",
    "python_df = pandas2ri.rpy2py(ro.globalenv['variable_name_in_r'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd5d389",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Grab the specific stock of interest\n",
    "aapl_data = enhanced_data['AAPL']\n",
    "# Create a Date column \n",
    "aapl_data = aapl_data.reset_index()\n",
    "\n",
    "# Check for any missing or null values \n",
    "print(aapl_data.isnull().sum())\n",
    "# Convert 'volume_obv' to a larger data type, like float64 because computer cannot store current type\n",
    "aapl_data['volume_obv'] = aapl_data['volume_obv'].astype(np.float64)\n",
    "\n",
    "# Now convert to R\n",
    "aapl_r_df = pandas2ri.py2rpy(aapl_data)\n",
    "\n",
    "# Assign df to the R environment for further use\n",
    "ro.globalenv['aapl_r_df'] = aapl_r_df\n",
    "\n",
    "# Set up R for complex statistical analysis \n",
    "ro.r('''\n",
    "    library(randomForest)  # For nonlinear relationships\n",
    "    library(MASS)          # For LTS, stepAIC\n",
    "    library(car)           # Companion to Applied Regression\n",
    "    library(forecast)      # Time Series forecasting\n",
    "    library(tseries)\n",
    "    library(lmtest)        # For regression diagnostics\n",
    "    library(ggplot2)       # For visualization\n",
    "    library(data.table)    # For fast data manipulation\n",
    "    library(dplyr)         # For data manipulation\n",
    "    library(boot)          # For bootstrap\n",
    "    library(glmnet)        # For ridge/lasso regression\n",
    "    library(quantreg)      # For quantile regression\n",
    "    library(leaps)         # For model selection, regsubsets\n",
    "    library(robustbase)    # For robust linear regression\n",
    "    library(caret)         # For cross-validation and model training\n",
    "    library(MCMCpack)      # For MCMC regression\n",
    "    library(mgcv)          # For generalized additive model\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56a3e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that we are operating on the desired data in R\n",
    "ro.r('''\n",
    "    summary <- summary(aapl_r_df)\n",
    "    print(summary)\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad51f27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first thing we want to do, is split the data into training and test set \n",
    "ro.r('''\n",
    "     aapl_r_df$Date <- as.Date(aapl_r_df$Date)\n",
    "     train_data <- aapl_r_df[aapl_r_df$Date < as.Date(\"2021-01-01\"), ]\n",
    "     test_data <- aapl_r_df[aapl_r_df$Date >= as.Date(\"2021-01-01\"), ]\n",
    "     # Show the first few rows of train dataset\n",
    "     print(head(train_data))\n",
    "     ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84df80ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ro.r(''' # Show first few rows of test dataset\n",
    "     print(head(test_data))\n",
    "     ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dec7052",
   "metadata": {},
   "outputs": [],
   "source": [
    "ro.r(''' # Check for missing values \n",
    "    sum(is.na(train_data))\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8594c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ro.r(''' # Convert to data.table \n",
    "    library(data.table)\n",
    "    setDT(train_data)  # Convert the train data to data.table\n",
    "    setDT(test_data)   # Convert the test data to data.table\n",
    "    class(train_data)  # Check to make sure conversion was done\n",
    "    \n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38da934e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ro.r('''\n",
    "    class(test_data)\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a043c4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ro.r('''\n",
    "    # Split the data into y ~ response and X ~ design matrix (predictors)\n",
    "     \n",
    "    # Response\n",
    "    response <- train_data[,.(Close)]\n",
    "    print(head(response))\n",
    "     \n",
    "    # Design Matrix  \n",
    "    predictors <- train_data[, !\"Close\", with = FALSE]\n",
    "    print(head(predictors))\n",
    "    \n",
    "    # For correlation and histogram creation \n",
    "    predictors <- as.data.frame(predictors)\n",
    "    \n",
    "     \n",
    "    ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74a012d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ro.r(''' # Data Cleaning \n",
    "    # Create response variable \n",
    "    response <- train_data[,.(Close)]\n",
    "    \n",
    "    # Create design matrix (predictors)\n",
    "    predictors <- train_data[,!'Close', with = FALSE]\n",
    "    predictors <- as.data.frame(predictors)\n",
    "     \n",
    "    response_df <- as.data.frame(response)\n",
    "    response_numeric <- as.numeric(train_data$Close)\n",
    "     \n",
    "    # Extract numeric columns from predictors\n",
    "    numeric_predictors <- predictors[, sapply(predictors, is.numeric)]\n",
    "     \n",
    "    # Verify to make sure non-numeric columns (like Date) have been removed  \n",
    "    print(head(numeric_predictors))\n",
    "     ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7eeedee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Perform the correlation-based filtering in R\n",
    "ro.r('''\n",
    "    # Initialize a vector to store relevant variable names\n",
    "    relevant_predictors <- c()\n",
    "    \n",
    "    # Loop through each predictor (column), calculate its correlation to the response\n",
    "    for (colname in colnames(numeric_predictors)) {    \n",
    "        # Correlation to response (Close)\n",
    "        correlation <- cor(numeric_predictors[[colname]], response$Close, use = \"complete.obs\")\n",
    "        \n",
    "        # If the absolute correlation meets the threshold, print and store\n",
    "        if (abs(correlation) > 0.2) {\n",
    "            cat(\"Analyzing:\", colname, \" \")\n",
    "            cat(\"Correlation with Close:\", correlation, \"\\n\")\n",
    "            # Append the relevant predictors to the vector\n",
    "            relevant_predictors <- c(relevant_predictors, colname)\n",
    "        } \n",
    "    }\n",
    "    \n",
    "    # Extract only the relevant predictors from numeric_predictors\n",
    "    corr_based_filtered_predictors <- numeric_predictors[, relevant_predictors, drop=FALSE]\n",
    "    print(head(corr_based_filtered_predictors))\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd13a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "ro.r(''' # Data parsing check \n",
    "       # Expecting 93\n",
    "       print(ncol(train_data))\n",
    "       # Expecting 92 \n",
    "       print(ncol(predictors))\n",
    "       # Expecting 91\n",
    "       print(ncol(numeric_predictors))\n",
    "       # Expecting 42\n",
    "       print(ncol(corr_based_filtered_predictors))\n",
    "       ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edfa611",
   "metadata": {},
   "outputs": [],
   "source": [
    "ro.r(''' # Number of missing values \n",
    "       sum(is.na(corr_based_filtered_predictors))\n",
    "       ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd8e0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rpy2.robjects as ro # Step 2: Generate histograms based on filtered predictors and the Close variable\n",
    "from rpy2.robjects.lib import grdevices\n",
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "def display_histograms():\n",
    "    # Fetch column names from the filtered predictors\n",
    "    colnames = list(ro.r('colnames(corr_based_filtered_predictors)'))\n",
    "    \n",
    "    # Add the 'Close' column separately as response\n",
    "    colnames.append(\"Close\")\n",
    "    \n",
    "    for colname in colnames:\n",
    "        colname_str = str(colname)  # Convert to Python string\n",
    "        filename = f\"histogram_{colname_str}.png\"  # Save each histogram as a PNG file\n",
    "        \n",
    "        # Determine which dataset to use (filtered predictors or response)\n",
    "        if colname_str == 'Close':\n",
    "            # For 'Close', use the numeric vector\n",
    "            dataset = \"response_numeric\"\n",
    "            ro.r(f'''\n",
    "            # Open a PNG plotting device in R\n",
    "            png(file=\"{filename}\", width=512, height=512)\n",
    "            # Generate the histogram using R\n",
    "            hist({dataset}, probability = TRUE,\n",
    "                 main=\"Histogram of Close\", \n",
    "                 xlab=\"Close\", \n",
    "                 col=\"lightblue\", \n",
    "                 border=\"black\")\n",
    "            # Add a normal distribution curve\n",
    "            curve(dnorm(x, mean=mean({dataset}, na.rm=TRUE), \n",
    "                 sd=sd({dataset}, na.rm=TRUE)), \n",
    "                 col=\"red\", lwd=2, add=TRUE)\n",
    "            dev.off()\n",
    "            ''')\n",
    "        else:\n",
    "            # For the predictors, use corr_based_filtered_predictors dataset\n",
    "            dataset = \"corr_based_filtered_predictors\"\n",
    "            ro.r(f'''\n",
    "            # Open a PNG plotting device in R\n",
    "            png(file=\"{filename}\", width=512, height=512)\n",
    "            # Generate the histogram using R\n",
    "            hist({dataset}[[\"{colname_str}\"]], probability = TRUE,\n",
    "                 main=paste(\"Histogram of\", \"{colname_str}\"), \n",
    "                 xlab=\"{colname_str}\", \n",
    "                 col=\"lightblue\", \n",
    "                 border=\"black\")\n",
    "            # Add a normal distribution curve\n",
    "            curve(dnorm(x, mean=mean({dataset}[[\"{colname_str}\"]], na.rm=TRUE), \n",
    "                 sd=sd({dataset}[[\"{colname_str}\"]], na.rm=TRUE)), \n",
    "                 col=\"red\", lwd=2, add=TRUE)\n",
    "            dev.off()\n",
    "            ''')\n",
    "        \n",
    "        # Display the histogram in Python\n",
    "        display(Image(filename))\n",
    "\n",
    "# Call the function to generate and display histograms\n",
    "display_histograms()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c837f504",
   "metadata": {},
   "source": [
    "Data Analysis Insight:\n",
    "After inspecting the histograms for the predictor variables, it is evident that the majority exhibit right-skewness. To stabilize the skewed distributions and improve normality, we will apply log transformations to these variables. This will allow us to better analyze the relationships between predictors and the response variable.\n",
    "\n",
    "Dynamic Programming Approach:\n",
    "To demonstrate flexibility and dynamic programming skills, I will perform the Box-Cox transformation analysis in Python. This approach enables me to systematically evaluate whether a log transformation or an alternative power transformation is appropriate for each variable, based on their distribution. By leveraging the interoperability between Python and R, I can efficiently switch between these languages to handle different aspects of the analysis and visualization to suit my personal strengths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a94802d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load R object into Python \n",
    "data = ro.r('corr_based_filtered_predictors')\n",
    "ncol_data = ro.r('colnames(corr_based_filtered_predictors)')\n",
    "\n",
    "# Convert R dataframe to a pandas dataframe using conversion\n",
    "with conversion.localconverter(ro.default_converter + pandas2ri.converter):\n",
    "    data = ro.conversion.rpy2py(data)\n",
    "\n",
    "# Ensure it's a pandas dataframe\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "# Create dictionaries to store columns and the data associated with columns\n",
    "positive_only_indicators = {}\n",
    "needs_log = {}\n",
    "no_log_transform = {}\n",
    "unsure_indicators = {}\n",
    "\n",
    "# Loop through each column in the dataframe\n",
    "for column in data.columns:\n",
    "    # Add small value to indicators that range from [0,100]\n",
    "    if (data[column] <= 0).any():\n",
    "        data[column] += 0.001\n",
    "    \n",
    "    # After adjusting, check if the column has only positive values for Box-Cox\n",
    "    if (data[column] > 0).all():\n",
    "        # Apply Box-Cox transformation\n",
    "        _, lambda_val = stats.boxcox(data[column].values)  # Use .values to pass as numpy array\n",
    "        positive_only_indicators[column] = data[column]  # Store the data for the positive-only indicator\n",
    "        \n",
    "        # Check if the lambda suggests log transformation (lambda ~ 0) or no transformation (lambda ~ 1)\n",
    "        if -0.125 < lambda_val < 0.125:\n",
    "            print(f\"{column} is best log transformed (lambda ~ 0)\")\n",
    "            needs_log[column] = data[column]  # Store the data for log transformation\n",
    "        elif 0.875 < lambda_val < 1.125:\n",
    "            print(f\"{column} does not need transformation (lambda ~ 1)\")\n",
    "            no_log_transform[column] = data[column]  # Store the data for no log transformation\n",
    "        else:\n",
    "            print(f\"{column} is best transformed with lambda: {lambda_val}\")\n",
    "            unsure_indicators[column] = data[column]  # Store the data for unsure indicators\n",
    "\n",
    "# Convert 'ncol_data' (an R vector) to a Python list\n",
    "ncol_data = list(ncol_data)\n",
    "\n",
    "# Find and print the names of the columns that are in 'ncol_data' but not in 'positive_only_indicators'\n",
    "non_positive_indicators = list(set(ncol_data) - set(positive_only_indicators.keys()))\n",
    "\n",
    "# Update unsure_indicators with non-positive columns\n",
    "for col in non_positive_indicators:\n",
    "    unsure_indicators[col] = data[col]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128ec320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expecting total number of indicators based on initial correlation filtering\n",
    "print(f\"Total number of correlation filtered indicators: {len(ncol_data)}\")\n",
    "# Expecting total number of indicators that are positive and pass initial filtering\n",
    "print(f\"Total number of positive-only indicators: {len(positive_only_indicators)}\")\n",
    "# Print number of non-positive indicators \n",
    "print(f\"Total number of non-positive indicators: {len(non_positive_indicators)}\")\n",
    "# Print number of non-positive indicators \n",
    "print(f\"Total number of indicators needing log transformation: {len(needs_log)}\")\n",
    "# Print number of no log transformation indicators \n",
    "print(f\"Total number of indicators with no log transformation: {len(no_log_transform)}\")\n",
    "# Print number of unsure indicators \n",
    "print(f\"Total number of indicators unsure: {len(unsure_indicators)}\")\n",
    "# Print names of non-positive indicators \n",
    "print(f\"Non-positive indicators:({non_positive_indicators})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530f8aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLog needing indicators:\")\n",
    "print('\\n'.join(needs_log.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bba9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nNo log transformation indicators:\")\n",
    "print('\\n'.join(no_log_transform.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfc49a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nUnsure indicators:\")\n",
    "print('\\n'.join(unsure_indicators.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449dfafe",
   "metadata": {},
   "source": [
    "Now we will evaluate the unsure indicators using skew and kurtosis and from here, fit a full model to finalize initial base model creation, having started with 90+ predictors for the response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3104c757",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in unsure_indicators:\n",
    "    print(f\"{column}: Skewness = {skew(data[column])}, Kurtosis = {kurtosis(data[column])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a480cc9",
   "metadata": {},
   "source": [
    "## Transformation Diagnosis\n",
    "\n",
    "### Skewness and Kurtosis Results \n",
    "\n",
    "- **Skewness in range [-0.5,0.5]** -> Fairly symmetric \n",
    "- **Kurtosis = 3** -> Normal distribution \n",
    "\n",
    "1. **High**: Highly right-skewed (1.89) with moderately heavy tails (3.75). **Apply log transformation.**\n",
    "2. **Volume**: Highly right-skewed (1.88) with heavy tails (5.02). **Apply log transformation.**\n",
    "3. **volatility_bbh**: Highly right-skewed (1.90) with moderately heavy tails (3.77). **Apply log transformation.**\n",
    "4. **volatility_bbw**: Moderately right-skewed (1.05) with light tails (1.37). **Transformation not critical, but log transformation may help normalize.**\n",
    "5. **volatility_kcw**: Highly right-skewed (2.03) with very heavy tails (7.04). **Apply log transformation and consider handling outliers.**\n",
    "6. **volatility_dch**: Highly right-skewed (1.94) with heavy tails (4.01). **Apply log transformation.**\n",
    "7. **trend_macd_signal**: Highly right-skewed (1.84) with extremely heavy tails (12.03). **Apply log transformation and handle outliers separately**.\n",
    "8. **volume_adi**: Highly left-skewed (-1.19) with near-normal tails (2.62). **Apply cube transformation or square transformation to reduce left skew.**\n",
    "9. **trend_kst_sig**: Extremely left-skewed (-4.60) with extremely heavy tails (32.65). **Apply square transformation and handle outliers.**\n",
    "10. **momentum_ppo**: Slightly left-skewed (-0.48) with light tails (0.68). **Transformation not necessary.**\n",
    "11. **momentum_ao**: Highly right-skewed (1.42) with extremely heavy tails (11.67). **Apply log transformation and handle outliers.**\n",
    "12. **momentum_ppo_signal**: Slightly left-skewed (-0.48) with light tails (0.70). **Transformation not necessary.**\n",
    "13. **volume_vpt**: Near symmetric (0.14) with light tails (0.63). **No transformation necessary.**\n",
    "14. **volume_obv**: Moderately left-skewed (-0.83) with light tails (1.65). **Apply square root transformation.**\n",
    "15. **trend_kst**: Extremely left-skewed (-4.41) with extremely heavy tails (32.38). **Apply square transformation and handle outliers.**\n",
    "16. **trend_macd**: Highly right-skewed (1.81) with extremely heavy tails (12.37). **Apply log transformation and manage outliers.**\n",
    "\n",
    "### Summary:\n",
    "- **Log transformations**: For most highly right-skewed variables.\n",
    "- **Square or cube transformations**: For highly left-skewed variables.\n",
    "- **Handle outliers**: For variables with extremely heavy tails (e.g., kurtosis > 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9ecbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rpy2.robjects as ro #  Data Preparation: Applying Transformations \n",
    "from rpy2.robjects.lib import grdevices\n",
    "from IPython.display import Image, display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rpy2.robjects import pandas2ri\n",
    "pandas2ri.activate()\n",
    "\n",
    "# Convert to df for easier analysis \n",
    "needs_log_df = pd.DataFrame(needs_log)\n",
    "no_log_transform_df = pd.DataFrame(no_log_transform)\n",
    "unsure_indicators_df = pd.DataFrame(unsure_indicators)\n",
    "\n",
    "# List of columns to add to needs_log_df (log transformation)\n",
    "additional_log_columns = [\"High\", \"Volume\", \"volatility_bbh\", \"volatility_dch\", \"volatility_kcw\"]\n",
    "\n",
    "# Add these columns to needs_log_df\n",
    "needs_log_df = pd.concat([needs_log_df, data[additional_log_columns]], axis=1)\n",
    "\n",
    "# List of all columns that need cube transformation instead of cube root\n",
    "cube_columns = [\"volume_adi\", \"volume_obv\", \"trend_kst_sig\", \"trend_kst\", \"momentum_ao\"]  \n",
    "\n",
    "# Apply cube transformation\n",
    "cube_df = data[cube_columns].apply(lambda x: np.power(x, 3))  # Cube transformation\n",
    "\n",
    "# List of columns to be added as normal predictors (no transformation needed)\n",
    "normal_predictors_columns = [\"volume_vpt\", \"momentum_ppo\", \"momentum_ppo_signal\", \"volatility_bbw\", \"trend_macd_signal\", \"trend_macd\"]\n",
    "\n",
    "# Create normal_predictors_df\n",
    "normal_predictors_df = data[normal_predictors_columns]\n",
    "\n",
    "# Apply log transformation to needs_log_df\n",
    "needs_log_df = needs_log_df.apply(np.log)\n",
    "\n",
    "# Combine all DataFrames (log transformed, cubed, and normal predictors)\n",
    "final_df = pd.concat([needs_log_df, cube_df, normal_predictors_df], axis=1)\n",
    "\n",
    "# Add the response variable and apply log transformation\n",
    "response_r = ro.r('train_data[[\"Close\"]]')  # Fetch the correct response from R\n",
    "response_r = np.log(response_r)  # Log transformation of the response\n",
    "\n",
    "response_r = pd.DataFrame(response_r)\n",
    "final_df['log_Close'] = response_r.to_numpy()[:, 0]\n",
    "\n",
    "# Drop any rows with missing values\n",
    "final_df = final_df.dropna(axis=0)\n",
    "\n",
    "# Ensure no columns are missing from the transformations\n",
    "all_transformed_columns = list(needs_log_df.columns) + list(cube_df.columns) + list(normal_predictors_df.columns)\n",
    "\n",
    "# Find and print the names of the columns that are tricky based on box cox, skewness, and kurtosis, likely due to outliers\n",
    "predictors_need_care = list(set(data.columns) - set(all_transformed_columns))\n",
    "\n",
    "# Check if predictors_need_care is empty\n",
    "if not predictors_need_care:\n",
    "    print(\"All columns have been transformed successfully.\")\n",
    "else:\n",
    "    print(f\"The following columns still need care: {predictors_need_care}\")\n",
    "    predictors_need_care_df = data[predictors_need_care]  # Create df for tricky predictors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c9ac77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Data Visualization: Renaming Columns and Displaying Histograms \n",
    "def display_transformed_histograms():\n",
    "    # Create a copy of final_df to avoid modifying the original data\n",
    "    transformed_df = final_df.copy()\n",
    "\n",
    "    # Get the column names from final_df\n",
    "    colnames = final_df.columns\n",
    "\n",
    "    # Loop through each column and apply transformation logic\n",
    "    for colname in colnames:\n",
    "        colname_str = str(colname)\n",
    "\n",
    "        # Determine the type of transformation applied\n",
    "        if colname_str in needs_log_df.columns:\n",
    "            transform_type = 'Log-Transformed'\n",
    "            new_colname = f\"log_{colname_str}\"\n",
    "            transformed_df.rename(columns={colname_str: new_colname}, inplace=True)\n",
    "        elif colname_str in cube_df.columns:\n",
    "            transform_type = 'Cube Transformed'\n",
    "            new_colname = f\"cube_{colname_str}\"\n",
    "            transformed_df.rename(columns={colname_str: new_colname}, inplace=True)\n",
    "        elif colname_str == \"log_Close\":  # Explicitly handle log_Close\n",
    "            transform_type = 'Log-Transformed'\n",
    "            new_colname = colname_str\n",
    "        else:\n",
    "            transform_type = 'Untransformed'\n",
    "            new_colname = colname_str\n",
    "\n",
    "        # Create a filename for the histogram\n",
    "        filename = f\"histogram_{new_colname}.png\"\n",
    "        \n",
    "        # Convert the transformed DataFrame to an R DataFrame\n",
    "        transformed_df_r = pandas2ri.py2rpy(transformed_df)\n",
    "\n",
    "        # Assign the R dataframe to a variable in R\n",
    "        ro.globalenv['transformed_df'] = transformed_df_r\n",
    "        \n",
    "        # Open a PNG plotting device in R\n",
    "        grdevices.png(file=filename, width=512, height=512)\n",
    "\n",
    "        # Prepare the R code for plotting\n",
    "        ro.r(f'''\n",
    "        # Extract the data column\n",
    "        data_vector <- transformed_df[[\"{new_colname}\"]]\n",
    "        \n",
    "        # Plot the histogram\n",
    "        hist(data_vector, probability = TRUE,\n",
    "             main=paste(\"{transform_type} Histogram of\", \"{colname}\"), \n",
    "             xlab=paste(\"{transform_type}\", \"{colname}\"), \n",
    "             col=\"lightblue\", \n",
    "             border=\"black\")\n",
    "        \n",
    "        # Add a normal distribution curve\n",
    "        curve(dnorm(x, mean=mean(data_vector, na.rm=TRUE), sd=sd(data_vector, na.rm=TRUE)), \n",
    "              col=\"red\", lwd=2, add=TRUE)\n",
    "        ''')\n",
    "        \n",
    "        # Close the PNG device\n",
    "        grdevices.dev_off()\n",
    "\n",
    "        # Display the histogram in Python\n",
    "        display(Image(filename))\n",
    "\n",
    "    return transformed_df\n",
    "\n",
    "# Call the function to display histograms and return the transformed DataFrame\n",
    "transformed_df = display_transformed_histograms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4155847",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df.head()\n",
    "# this is as desired \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68a98ae",
   "metadata": {},
   "source": [
    "Cube Root transformations were not good, went back and did Cube transformations, these were better. Some variables likely have outliers making simple transformations ineffective. We need to take a closer look at these variables later, potentiall using robust regression methods to take care of outliers. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eae2ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert needs_log_df to R DataFrame and move to R environment\n",
    "needs_log_df_r = pandas2ri.py2rpy(needs_log_df.dropna()) # Use dropna to handle potential errors \n",
    "ro.globalenv['needs_log_df'] = needs_log_df_r\n",
    "\n",
    "ro.globalenv['final_df'] = final_df\n",
    "\n",
    "print(ro.r('head(needs_log_df)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e856a976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert normal_predictors_df to R DataFrame and move to R environment\n",
    "normal_predictors_df_r = pandas2ri.py2rpy(normal_predictors_df.dropna()) # Use dropna to handle potential errors \n",
    "ro.globalenv['normal_predictors_df'] = normal_predictors_df_r\n",
    "print(ro.r('head(normal_predictors_df)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a090462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert cube_root_df to R DataFrame and move to R environment\n",
    "cube_df_r = pandas2ri.py2rpy(cube_df.dropna()) # Use dropna to handle potential errors \n",
    "ro.globalenv['cube_df'] = cube_df_r\n",
    "print(ro.r('head(cube_df)'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8745e194",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ro.r('head(final_df)')) # In R, final_df has log_Close "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482422ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ro.r('head(transformed_df)')) # In R, tranformed_df adjusts column names appropriately\n",
    "\n",
    "# Transformed response and predictor\n",
    "print(ro.r('ncol(transformed_df)'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e774f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the test_data from the R environment with adjustments \n",
    "test_data = pandas2ri.rpy2py(ro.globalenv['test_data'])\n",
    "\n",
    "# Function to apply the same transformations to the test set\n",
    "def transform_test_set(test_data):\n",
    "    # Create a copy of test_data to avoid modifying the original test data\n",
    "    transformed_test_df = test_data.copy()\n",
    "\n",
    "    # Get the column names from the training set (transformed_df)\n",
    "    colnames = transformed_df.columns  # This includes the transformed names\n",
    "\n",
    "    # Apply transformations to the test set based on training set transformations\n",
    "    for colname in test_data.columns:\n",
    "        colname_str = str(colname)\n",
    "        \n",
    "        # Check if the column was log-transformed in the training set\n",
    "        if f\"log_{colname_str}\" in colnames:\n",
    "            # Apply log transformation and rename the column\n",
    "            transformed_test_df[f\"log_{colname_str}\"] = np.log(transformed_test_df[colname_str])\n",
    "            transformed_test_df.drop(columns=[colname_str], inplace=True)  # Drop the original column\n",
    "        \n",
    "        # Check if the column was cube root-transformed in the training set\n",
    "        elif f\"cube_{colname_str}\" in colnames:\n",
    "            # Apply cube root transformation and rename the column\n",
    "            transformed_test_df[f\"cube_{colname_str}\"] = transformed_test_df[colname_str].apply(lambda x: np.power(x, 3))\n",
    "                                                                                                 \n",
    "            transformed_test_df.drop(columns=[colname_str], inplace=True)  # Drop the original column\n",
    "\n",
    "    # Return the transformed test set\n",
    "    return transformed_test_df\n",
    "\n",
    "# Apply the function to transform the test set\n",
    "transformed_test_df = transform_test_set(test_data)\n",
    "\n",
    "print(transformed_test_df.head())\n",
    "# Now the test set (transformed_test_df) has the same transformations and column names as the training set (transformed_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739c8935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert final, transformed test df to an R DataFrame\n",
    "transformed_test_df_r = pandas2ri.py2rpy(transformed_test_df)\n",
    "\n",
    "# Assign the R dataframe to a variable in R\n",
    "ro.globalenv['transformed_test_df'] = transformed_test_df_r\n",
    "\n",
    "ro.r('''\n",
    "     \n",
    "    transformed_test_df = as.data.table(transformed_test_df)\n",
    "    print(head(transformed_test_df))\n",
    "     \n",
    "    ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d5a286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure transformed_test_df_clean only contains columns present in transformed_df\n",
    "def align_columns(df_source, df_target):\n",
    "    # Find common columns between source and target\n",
    "    common_columns = df_source.columns.intersection(df_target.columns)\n",
    "    # Align target dataframe to source dataframe's common columns\n",
    "    df_target_aligned = df_target[common_columns]\n",
    "    return df_target_aligned\n",
    "\n",
    "# Apply the alignment to both training and test sets\n",
    "transformed_test_df_clean = align_columns(transformed_df, transformed_test_df)\n",
    "\n",
    "# Print the first few rows of the aligned test dataframe\n",
    "print(transformed_test_df_clean.head())\n",
    "\n",
    "# Check the new shapes of the aligned dataframes\n",
    "print(\"Training set number of columns:\", transformed_df.shape[1])\n",
    "print(\"Test set number of aligned columns:\", transformed_test_df_clean.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cc2a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This has everything as desired, all predictors and the response based on correlation filtering \n",
    "print(ro.r('ncol(transformed_df)'))\n",
    "\n",
    "# This does not yet \n",
    "print(ro.r('ncol(transformed_test_df)'))\n",
    "\n",
    "# Transform the R dataframe to reflect the changes made in Python \n",
    "ro.globalenv['transformed_test_df'] = transformed_test_df_clean\n",
    "print(ro.r('ncol(transformed_test_df)'))\n",
    "\n",
    "# Problem has been resolved \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e033ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ro.r(''' # Fit a linear model with all predictors \n",
    "    # Fit a linear model with all predictors \n",
    "    full_model <- lm(log_Close ~ ., data = transformed_df)\n",
    "    # Grab summary statistics \n",
    "    print(summary(full_model))\n",
    "''')\n",
    "\n",
    "\n",
    "ro.r('''\n",
    "    set.seed(123)\n",
    "    # Exponentiate the values in the test set to get them back on the real scale\n",
    "    y_test <- transformed_test_df$log_Close \n",
    "    exp_y_test <- exp(transformed_test_df$log_Close)\n",
    "    \n",
    "    # Use the full model to predict on the test data\n",
    "    predictions_full_test <- predict(full_model, newdata = transformed_test_df)\n",
    "    \n",
    "    # Exponentiate the predicted values to get them on the real scale\n",
    "    predictions_full_test_exp <- exp(predictions_full_test)\n",
    "\n",
    "    # Apply smearing correction to account for bias\n",
    "    residuals_full <- y_test - predictions_full_test  # Residuals on the log scale\n",
    "    correction_factor <- mean(exp(residuals_full))  # Smearing correction factor\n",
    "    predictions_full_test_exp_smearing <- predictions_full_test_exp * correction_factor\n",
    "\n",
    "    # Calculate Full Model MSPE on the log scale (without exponentiating)\n",
    "    full_model_mspe_log <- mean((y_test - predictions_full_test)^2)\n",
    "\n",
    "    # Calculate Full Model MSPE on the real scale (after smearing correction)\n",
    "    full_model_mspe_real <- mean((exp_y_test - predictions_full_test_exp_smearing)^2)\n",
    "\n",
    "    # Print the results\n",
    "    cat(\"Full Model MSPE (Log Scale):\", full_model_mspe_log, \"\\n\")\n",
    "    cat(\"Full Model MSPE (Real Scale, Smearing Corrected):\", full_model_mspe_real, \"\\n\")\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3257b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "ro.r(''' # Calculate VIF for all predictors\n",
    "    # VIF calculation function is in the 'car' package\n",
    "    library(car)  \n",
    "    \n",
    "    # Calculate VIF for the full model\n",
    "    vif_values <- vif(full_model)\n",
    "    \n",
    "    # Identify variables with high VIF \n",
    "    high_vif <- names(vif_values[vif_values > 10])\n",
    "    \n",
    "    # Identify variables with low VIF\n",
    "    low_vif <- names(vif_values[vif_values <= 10])\n",
    "     \n",
    "    # Display results  \n",
    "    cat(\"Variables with high VIF:\\n\")\n",
    "    print(high_vif) \n",
    "    cat(\"Variables with low VIF:\\n\")\n",
    "    print(low_vif)\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3229c3",
   "metadata": {},
   "source": [
    "We will look at the model that results from VIF to see how it is. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35feb50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ro.r(''' # Fit a linear model with VIF predictors\n",
    "    vif_model <- lm(log_Close ~ log_Volume + cube_volume_adi + cube_volume_obv + volume_vpt + cube_momentum_ao, data = transformed_df)\n",
    "    # Print summary statistics\n",
    "    print(summary(vif_model))\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45330654",
   "metadata": {},
   "outputs": [],
   "source": [
    "ro.r(''' # Get the actual test values in log scale \n",
    "    y_test <- transformed_test_df$log_Close\n",
    "    exp_y_test <- exp(transformed_test_df$log_Close)\n",
    "\n",
    "    # Predict on the test data using the VIF model\n",
    "    predictions_test <- predict(vif_model, newdata = transformed_test_df)\n",
    "\n",
    "    # Exponentiate the predictions to get them on the real scale\n",
    "    predictions_test_exp <- exp(predictions_test)\n",
    "\n",
    "    # Calculate MSPE for the log scale (no need to exponentiate)\n",
    "    vif_mspe <- mean((y_test - predictions_test)^2)\n",
    "\n",
    "    # Calculate MSPE for the real scale\n",
    "    vif_real_mspe <- mean((exp_y_test - predictions_test_exp)^2)\n",
    "\n",
    "    # Print the results\n",
    "    cat(\"VIF MSPE (Log Scale):\", vif_mspe, \"\\n\")\n",
    "    cat(\"Full Model MSPE (Log Scale):\", full_model_mspe_log, \"\\n\")\n",
    " \n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2abaa2f",
   "metadata": {},
   "source": [
    "### Model Performance: Full Model vs. VIF Model\n",
    "\n",
    "We evaluated two regression models for predicting financial data: the **full model** (with all available transformed and untransformed predictors based on correlation filtering) and a **simplified VIF model** (with predictors filtered using correlation, histogram analysis, and VIF elimination). Both models were trained on log-transformed data and tested on unseen data to assess their generalization abilities.\n",
    "\n",
    "#### Methodology:\n",
    "- **Full Model**: Used all predictors, capturing a broader range of relationships.\n",
    "- **VIF Model**: Utilized a reduced set of predictors, focusing on minimizing multicollinearity and simplifying the model.\n",
    "\n",
    "#### Test Set Performance:\n",
    "- **VIF Model MSPE**:\n",
    "  - Log Scale: `0.0834`\n",
    "  \n",
    "  \n",
    "- **Full Model MSPE**:\n",
    "  - Log Scale: `4.23e-05`\n",
    "  \n",
    "\n",
    "#### Conclusion:\n",
    "The **full model** significantly outperformed the VIF model, especially on the real scale. This is likely due to the full model’s ability to capture the inherent complexities and subtle relationships present in financial data. While the VIF model was simplified to reduce multicollinearity, it likely missed important predictors, leading to higher prediction error on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b380d8d",
   "metadata": {},
   "source": [
    "## Regularization Methods \n",
    "\n",
    "Now I will do regularized regression methods, Lasso, Ridge, and ElasticNet. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f78329",
   "metadata": {},
   "outputs": [],
   "source": [
    "ro.r(''' # Ridge \n",
    "# Load necessary libraries\n",
    "library(glmnet)\n",
    "library(caret)\n",
    "     \n",
    "# For reproducibility \n",
    "set.seed(123)\n",
    "\n",
    "# transformed_df uses the training data \n",
    "# Convert predictors and response using model.matrix\n",
    "x <- model.matrix(log_Close ~ . - 1, transformed_df)  # Create design matrix, excluding intercept \n",
    "y <- transformed_df$log_Close  # The response variable \n",
    "     \n",
    "\n",
    "# Ridge Regression (alpha = 0)\n",
    "ridge_model <- cv.glmnet(x, y, alpha = 0)\n",
    "print(\"Ridge Regression:\")\n",
    "print(ridge_model)\n",
    "     \n",
    "# Optimal lambda for the Ridge model \n",
    "best_lambda_ridge <- ridge_model$lambda.min\n",
    "cat(\"Best lambda for Ridge:\", best_lambda_ridge, \"\\\\n\")\n",
    "\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a01093",
   "metadata": {},
   "outputs": [],
   "source": [
    "ro.r(''' # Ridge Coefficients \n",
    "set.seed(123)\n",
    "# Get Ridge coefficients\n",
    "ridge_coefs <- coef(ridge_model, s = best_lambda_ridge)\n",
    "print(\"Ridge Coefficients:\")\n",
    "print(ridge_coefs)\n",
    "     ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557ba86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ro.r(''' # Lasso \n",
    "set.seed(123)\n",
    "# Lasso Regression (alpha = 1)\n",
    "lasso_model <- cv.glmnet(x, y, alpha = 1)\n",
    "print(\"Lasso Regression:\")\n",
    "print(lasso_model)\n",
    "     \n",
    "# Optimal lambda for the Ridge model \n",
    "best_lambda_lasso <- lasso_model$lambda.min\n",
    "cat(\"Best lambda for Lasso:\", best_lambda_lasso, \"\\\\n\")\n",
    "     ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4109b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ro.r(''' # Coefficients from the Lasso model\n",
    "lasso_coefs <- coef(lasso_model, s = best_lambda_lasso)\n",
    "print(lasso_coefs)\n",
    "     ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f2b62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ro.r('''   # ElasticNet   \n",
    "# Set seed for reproducibility\n",
    "set.seed(123)\n",
    "\n",
    "x <- model.matrix(log_Close ~ . - 1, transformed_df)  # Create design matrix, excluding intercept \n",
    "y <- transformed_df$log_Close  # The response variable \n",
    "\n",
    "     \n",
    "# Define the alpha values to loop over (e.g., from 0 to 1 in steps of 0.1)\n",
    "alpha_values <- seq(0, 1, by = 0.1)\n",
    "\n",
    "# Initialize variables to store the best results\n",
    "best_alpha <- NULL\n",
    "best_lambda <- NULL\n",
    "lowest_mse <- Inf  # Set initial MSE to infinity\n",
    "\n",
    "# Loop through each alpha value\n",
    "for (alpha_value in alpha_values) {\n",
    "    # Fit ElasticNet model for each alpha\n",
    "    elasticnet_model <- cv.glmnet(x, y, alpha = alpha_value)\n",
    "    \n",
    "    # Get the best lambda for this alpha\n",
    "    best_lambda_for_alpha <- elasticnet_model$lambda.min\n",
    "    \n",
    "    # Predict using the best lambda\n",
    "    predictions <- predict(elasticnet_model, s = best_lambda_for_alpha, newx = x)\n",
    "    \n",
    "    # Calculate MSE\n",
    "    mse <- mean((y - predictions)^2)\n",
    "    \n",
    "    # Output the appropriate message for each model type\n",
    "    if (alpha_value == 0){\n",
    "        cat(\"Ridge Regression: MSE:\", mse, \" | Best Lambda:\", best_lambda_for_alpha, \"\\n\")\n",
    "    } else if (alpha_value == 1){\n",
    "        cat(\"Lasso Regression: MSE:\", mse, \" | Best Lambda:\", best_lambda_for_alpha, \"\\n\")\n",
    "    } else {\n",
    "        cat(\"Alpha:\", alpha_value, \" | MSE:\", mse, \" | Best Lambda:\", best_lambda_for_alpha, \"\\n\")\n",
    "    }\n",
    "    \n",
    "    # Update the best alpha and lambda if this model has the lowest MSE\n",
    "    if (mse < lowest_mse) {\n",
    "        best_alpha <- alpha_value\n",
    "        best_lambda <- best_lambda_for_alpha\n",
    "        lowest_mse <- mse \n",
    "    }\n",
    "}\n",
    "\n",
    "# Print the best alpha, lambda, and corresponding MSE\n",
    "cat(\"\\nBest Alpha:\", best_alpha, \"\\n\")\n",
    "cat(\"Best Lambda:\", best_lambda, \"\\n\")\n",
    "cat(\"Lowest MSE:\", lowest_mse, \"\\n\")\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4d92a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ro.r(''' # Coefficients from the ElasticNet model\n",
    "elasticnet_coefs <- coef(elasticnet_model, s = best_lambda_enet)\n",
    "print(elasticnet_coefs)\n",
    "     ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18c5d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "ro.r(''' # MSPE \n",
    "set.seed(123)\n",
    "\n",
    "# Check and align test data to match training data columns\n",
    "transformed_test_df <- transformed_test_df[, colnames(transformed_df), drop = FALSE]\n",
    "\n",
    "# Compute MSPE on the test set\n",
    "test_x <- model.matrix(log_Close ~ . - 1, transformed_test_df)  # Prepare test data\n",
    "test_y <- transformed_test_df$log_Close  # True values from the test set\n",
    "\n",
    "# Use best lambda and model (ElasticNet, Ridge, or Lasso)\n",
    "elasticnet_predictions <- predict(elasticnet_model, s = best_lambda, newx = test_x)\n",
    "ridge_predictions <- predict(ridge_model, s = best_lambda_ridge, newx = test_x)\n",
    "lasso_predictions <- predict(lasso_model, s = best_lambda_lasso, newx = test_x)\n",
    "vif_predictions <- predict(vif_model, newdata = transformed_test_df)  # VIF model uses full test data\n",
    "\n",
    "# Calculate MSPE for each method\n",
    "elasticnet_mspe <- mean((test_y - elasticnet_predictions)^2)\n",
    "ridge_mspe <- mean((test_y - ridge_predictions)^2)\n",
    "lasso_mspe <- mean((test_y - lasso_predictions)^2)\n",
    "vif_mspe <- mean((test_y - vif_predictions)^2)\n",
    "\n",
    "# Output the results\n",
    "cat(\"ElasticNet MSPE (Log Scale):\", elasticnet_mspe, \"\\n\")\n",
    "cat(\"Ridge MSPE (Log Scale):\", ridge_mspe, \"\\n\")\n",
    "cat(\"Lasso MSPE (Log Scale):\", lasso_mspe, \"\\n\")\n",
    "cat(\"VIF MSPE (Log Scale):\", vif_mspe, \"\\n\")\n",
    "cat(\"Full Model MSPE (Log Scale):\", full_model_mspe_log, \"\\n\")\n",
    "\n",
    "library(MASS)\n",
    "# Fit robust regression model\n",
    "robust_model <- rlm(log_Close ~ . , data = transformed_df)\n",
    "# Predict on the test data\n",
    "robust_preds <- predict(robust_model, newdata = transformed_test_df)\n",
    "robust_mspe <- mean((test_y - robust_preds)^2)\n",
    "cat(\"Huber Regression MSPE (Log Scale):\", robust_mspe, \"\\n\")\n",
    "\n",
    "# Fit the LTS regression model\n",
    "lts_model <- lqs(log_Close ~ ., data = transformed_df)\n",
    "lts_preds <- predict(lts_model, newdata = transformed_test_df)\n",
    "lts_mspe <- mean((test_y - lts_preds)^2)\n",
    "cat(\"Least Trimmed Squares Regression MSPE (Log Scale):\", lts_mspe, \"\\n\")\n",
    "\n",
    "# Fit the quantile regression model at the median (tau = 0.5)\n",
    "library(quantreg)\n",
    "qr_model <- rq(log_Close ~ . , data = transformed_df, tau = 0.5)\n",
    "qr_preds <- predict(qr_model, newdata = transformed_test_df)\n",
    "qr_mspe <- mean((test_y - qr_preds)^2)\n",
    "cat(\"Quantile Regression MSPE (Log Scale):\", qr_mspe, \"\\n\")\n",
    "\n",
    "# Fit penalized quantile regression\n",
    "library(rqPen)\n",
    "pen_qr_model <- rq.fit.lasso(x = test_x, y = test_y, tau = 0.5)\n",
    "pen_qr_preds <- test_x %*% pen_qr_model$coefficients\n",
    "pen_qr_mspe <- mean((test_y - pen_qr_preds)^2)\n",
    "cat(\"Penalized Quantile Regression MSPE (Log Scale):\", pen_qr_mspe, \"\\n\")\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2857,
   "id": "5f260775",
   "metadata": {},
   "outputs": [],
   "source": [
    "ro.r(''' # MSPE Real Scale using bias correction \n",
    "set.seed(123)\n",
    "\n",
    "# Check and align test data to match training data columns\n",
    "transformed_test_df <- transformed_test_df[, colnames(transformed_df), drop = FALSE]\n",
    "\n",
    "# Compute MSPE on the test set\n",
    "test_x <- model.matrix(log_Close ~ . - 1, transformed_test_df)  # Prepare test data\n",
    "test_y <- transformed_test_df$log_Close  # True values from the test set\n",
    "\n",
    "# Smearing function to apply bias correction\n",
    "smearing_bias_correction <- function(log_preds, log_true) {\n",
    "  residuals_log <- log_true - log_preds\n",
    "  correction_factor <- mean(exp(residuals_log))  # Smearing correction factor\n",
    "  return(correction_factor)\n",
    "}\n",
    "\n",
    "# Function to calculate MSPE on the real scale\n",
    "calculate_mspe_original <- function(log_preds, log_true) {\n",
    "  pred_original <- exp(log_preds)\n",
    "  true_original <- exp(log_true)\n",
    "  \n",
    "  # Apply smearing bias correction\n",
    "  correction_factor <- smearing_bias_correction(log_preds, log_true)\n",
    "  adjusted_preds <- pred_original * correction_factor\n",
    "  \n",
    "  # Calculate MSPE on the original scale\n",
    "  mspe_original <- mean((true_original - adjusted_preds)^2)\n",
    "  return(mspe_original)\n",
    "}\n",
    "\n",
    "# ElasticNet predictions and MSPE\n",
    "elasticnet_predictions <- predict(elasticnet_model, s = best_lambda, newx = test_x)\n",
    "elasticnet_mspe_real <- calculate_mspe_original(elasticnet_predictions, test_y)\n",
    "\n",
    "# Ridge predictions and MSPE\n",
    "ridge_predictions <- predict(ridge_model, s = best_lambda_ridge, newx = test_x)\n",
    "ridge_mspe_real <- calculate_mspe_original(ridge_predictions, test_y)\n",
    "\n",
    "# Lasso predictions and MSPE\n",
    "lasso_predictions <- predict(lasso_model, s = best_lambda_lasso, newx = test_x)\n",
    "lasso_mspe_real <- calculate_mspe_original(lasso_predictions, test_y)\n",
    "\n",
    "# VIF Model predictions and MSPE\n",
    "vif_predictions <- predict(vif_model, newdata = transformed_test_df)  # VIF model uses full test data\n",
    "vif_mspe_real <- calculate_mspe_original(vif_predictions, test_y)\n",
    "\n",
    "# Full Model MSPE (Real Scale)\n",
    "full_model_mspe_real <- calculate_mspe_original(predictions_full_test, test_y)\n",
    "\n",
    "# Robust Model MSPE\n",
    "robust_preds <- predict(robust_model, newdata = transformed_test_df)\n",
    "robust_mspe_real <- calculate_mspe_original(robust_preds, test_y)\n",
    "\n",
    "# LTS Model MSPE\n",
    "lts_preds <- predict(lts_model, newdata = transformed_test_df)\n",
    "lts_mspe_real <- calculate_mspe_original(lts_preds, test_y)\n",
    "\n",
    "# Quantile Regression Model MSPE\n",
    "qr_preds <- predict(qr_model, newdata = transformed_test_df)\n",
    "qr_mspe_real <- calculate_mspe_original(qr_preds, test_y)\n",
    "\n",
    "# Penalized Quantile Regression Model MSPE\n",
    "pen_qr_preds <- test_x %*% pen_qr_model$coefficients\n",
    "pen_qr_mspe_real <- calculate_mspe_original(pen_qr_preds, test_y)   \n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeed2ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ro.r(''' # Output the results (Real Scale)\n",
    "cat(\"ElasticNet MSPE (Real Scale):\", elasticnet_mspe_real, \"\\n\")\n",
    "cat(\"Ridge MSPE (Real Scale):\", ridge_mspe_real, \"\\n\")\n",
    "cat(\"Lasso MSPE (Real Scale):\", lasso_mspe_real, \"\\n\")\n",
    "cat(\"VIF MSPE (Real Scale):\", vif_mspe_real, \"\\n\")\n",
    "cat(\"Full Model MSPE (Real Scale):\", full_model_mspe_real, \"\\n\")\n",
    "cat(\"Huber Regression MSPE (Real Scale):\", robust_mspe_real, \"\\n\")\n",
    "cat(\"Least Trimmed Squares MSPE (Real Scale):\", lts_mspe_real, \"\\n\")\n",
    "cat(\"Quantile Regression MSPE (Real Scale):\", qr_mspe_real, \"\\n\")\n",
    "cat(\"Penalized Quantile Regression MSPE (Real Scale):\", pen_qr_mspe_real, \"\\n\")  \n",
    "     ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9da5b28",
   "metadata": {},
   "source": [
    "# Analysis of Robust and Regularized Regression Methods\n",
    "\n",
    "In my analysis, I applied robust regression methods—Quantile Regression (QR), Least Trimmed Squares (LTS), and Huber's method—and regularized regression methods—Lasso, Ridge, and Elastic Net regression models. My goal was to address outliers and multicollinearity, evaluating the predictive performance of these regression models.\n",
    "\n",
    "## Results\n",
    "\n",
    "### Robust Regression Methods\n",
    "\n",
    "| **Model**                          | **MSPE (Log Scale)** | **MSPE (Original Scale)** |\n",
    "|------------------------------------|----------------------|---------------------------|\n",
    "| Quantile Regression                | 3.6021e-05           | 0.8046161                 |\n",
    "| Huber Regression                   | 3.789034e-05         | 0.8522115                 |\n",
    "| Penalized Quantile Regression      | 7.939872e-05         | 1.758258                  |\n",
    "| Least Trimmed Squares Regression   | 0.0007393806         | 15.15739                  |\n",
    "\n",
    "### Regularized Regression Methods\n",
    "\n",
    "| **Model**                  | **MSPE (Log Scale)** | **MSPE (Original Scale)** |\n",
    "|----------------------------|----------------------|---------------------------|\n",
    "| Ridge Regression            | 0.0006943541         | 16.37199                  |\n",
    "| Lasso Regression            | 0.002135133          | 4.930664                  |\n",
    "| Elastic Net Regression      | 0.002135133          | 4.930664                  |\n",
    "| VIF Model                   | 0.08338693           | 3076.539                  |\n",
    "| Full Model                  | 4.225889e-05         | 0.9543641                 |\n",
    "\n",
    "## Interpretation and Conclusions\n",
    "\n",
    "### Robust Regression Methods\n",
    "\n",
    "- **Quantile Regression (QR)** had the lowest MSPE on both the log and original scales, with **3.6021e-05 (Log)** and **0.8046161 (Original)**, making it the top performer.\n",
    "\n",
    "- **Huber Regression** followed closely, with an MSPE of **3.789034e-05 (Log)** and **0.8522115 (Original)**, indicating its robustness against outliers.\n",
    "\n",
    "- **Penalized Quantile Regression** showed a higher MSPE than QR and Huber, with **7.939872e-05 (Log)** and **1.758258 (Original)**.\n",
    "\n",
    "- **Least Trimmed Squares (LTS) Regression** performed worst among robust methods, with **0.0007393806 (Log)** and **15.15739 (Original)**.\n",
    "\n",
    "### Regularized Regression Methods\n",
    "\n",
    "- **Ridge Regression** had a relatively low MSPE on the log scale (**0.0006943541**) but was less effective on the original scale (**16.37199**).\n",
    "\n",
    "- **Lasso** and **Elastic Net Regression** showed similar performance, with **0.002135133 (Log)** and **4.930664 (Original)**.\n",
    "\n",
    "- The **VIF Model** performed the worst, with an MSPE of **0.08338693 (Log)** and **3076.539 (Original)**, indicating suboptimal predictions.\n",
    "\n",
    "- The **Full Model** was the strongest regularized method, with **4.225889e-05 (Log)** and **0.9543641 (Original)**.\n",
    "\n",
    "## Mathematical Reasoning\n",
    "\n",
    "- **Quantile Regression (QR)** is effective at estimating the median relationship between the predictors and the response. It minimizes the influence of extreme values by focusing on the median rather than the mean, which explains its superior performance on both scales.\n",
    "\n",
    "- **Huber Regression** combines squared error for small residuals and absolute error for large residuals. This approach makes it resistant to outliers while preserving the efficiency of least squares for typical data points.\n",
    "\n",
    "- **Regularized Regression (Ridge, Lasso, and Elastic Net)** applies penalties to the regression coefficients. Ridge regression uses the L2 penalty to shrink coefficients of correlated predictors, while Lasso applies an L1 penalty to enforce sparsity. Elastic Net is a combination of both L1 and L2 penalties, balancing shrinkage and variable selection.\n",
    "\n",
    "- The **VIF Model** was built by removing predictors with high multicollinearity based on the variance inflation factor. However, this simplistic approach likely resulted in the removal of useful predictors, leading to poor predictive performance.\n",
    "\n",
    "## Practical Reasoning\n",
    "\n",
    "Financial data often contains extremes and high volatility, which can distort traditional models. Robust methods like Quantile and Huber regression are less influenced by outliers, making them well-suited to handle the irregularities in financial data. This is why these models performed better than regularized methods, which may be more sensitive to such fluctuations.\n",
    "\n",
    "## Takeaways\n",
    "\n",
    "1. **Quantile Regression and Huber Regression outperformed the regularized methods**, making them the most effective for this dataset.\n",
    "2. **Ridge Regression performed well on the log scale**, but it struggled when transformed back to the original scale.\n",
    "3. **The Full Model showed the best performance among regularized methods**, while the **VIF Model** underperformed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1316e841",
   "metadata": {},
   "source": [
    "# Transition to Time Series Analysis (TSA)\n",
    "\n",
    "Following these analyses, the next logical step is to incorporate time-dependent patterns through **Time Series Analysis (TSA)**. By applying **ARIMA** and **GARCH** models, we aim to capture autocorrelation and volatility clustering, respectively. This transition will allow us to account for the temporal structure in stock prices and improve model performance further.\n",
    "## Time Series Analysis (TSA)\n",
    "\n",
    "While the regression models provided valuable insights, they may not fully capture temporal dependencies inherent in time series data. **Time Series Analysis** techniques are specifically designed to model and forecast data where observations are correlated over time.\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "1. **Exploratory Time Series Analysis:** \n",
    "   - Visualize the time series plots of the response variable to identify any underlying **trends**, **seasonality**, and **autocorrelation** patterns.\n",
    "   - Use tools like the **Autocorrelation Function (ACF)** and **Partial Autocorrelation Function (PACF)** plots to help determine the order of the AR and MA components in an ARIMA model.\n",
    "\n",
    "2. **Stationarity Assessment:** \n",
    "   - Apply stationarity tests such as the **Augmented Dickey-Fuller (ADF) test** to check for stationarity.\n",
    "   - If the data is non-stationary, use **differencing** or transformations (e.g., log transformations) to achieve stationarity.\n",
    "\n",
    "3. **Modeling Temporal Dependencies:** \n",
    "   - Fit **ARIMA (AutoRegressive Integrated Moving Average)** or **SARIMA (Seasonal ARIMA)** models to capture autocorrelation and other temporal structures.\n",
    "   - Consider integrating a **GARCH (Generalized Autoregressive Conditional Heteroskedasticity)** model to handle volatility clustering and model variance over time.\n",
    "\n",
    "4. **Forecast Evaluation:** \n",
    "   - Compare the forecast performance of TSA models (e.g., ARIMA, SARIMA, GARCH) with the static regression models using **forecast accuracy metrics** like **Mean Squared Prediction Error (MSPE)**, **Mean Absolute Error (MAE)**, and **Mean Absolute Percentage Error (MAPE)**.\n",
    "\n",
    "By incorporating Time Series Analysis techniques, I will better understand the dynamics of the data over time, leading to more accurate and robust forecasts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c0173a",
   "metadata": {},
   "source": [
    "### Further Justifications\n",
    "\n",
    "Given the inherent autocorrelation in stock market data, the next step is to conduct Time Series Analysis (TSA) to capture the temporal patterns. By applying ARIMA and potentially integrating GARCH models, we aim to model both the time-dependent trends and volatility in the data. This should help alleviate the burden on other predictors, allowing them to better capture the underlying relationships.\n",
    "\n",
    "Once TSA is incorporated, we will re-evaluate the performance of robust and regularized regression methods, now accounting for time-series noise. Following TSA, we plan to explore Principal Component Analysis (PCA) and tree-based models (e.g., Random Forests) to further address multicollinearity, improve feature selection, and enhance prediction accuracy.\n",
    "\n",
    "This approach ensures a comprehensive model that leverages both temporal dynamics and feature-based insights for more robust predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7962ae7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to save R plots to a PNG file and display them in Python\n",
    "\n",
    "from IPython.display import Image, display\n",
    "def display_r_plot(filename):\n",
    "    display(Image(filename))\n",
    "\n",
    "\n",
    "ro.r('''\n",
    "# Convert the 'Close' column to a time series object\n",
    "close_ts <- ts(aapl_r_df$Close, start = c(2009, 1), frequency = 252) # Assuming daily data with ~252 trading days per year\n",
    "     \n",
    "# Transform the response variable as we did previously from Exploratory Analysis \n",
    "close_ts <- log(close_ts)\n",
    "# Verify we have created a time series object \n",
    "print(class(close_ts))\n",
    "\n",
    "# Create a new column in the dataframe to hold log_close      \n",
    "aapl_r_df$log_Close <- log(aapl_r_df$Close)\n",
    "# Check the basic structure of the time series data\n",
    "print(summary(close_ts))\n",
    "plot(close_ts, main = \"Close Price Time Series\", ylab = \"Price\", xlab = \"Time\")\n",
    "    ''')\n",
    "\n",
    "ro.r('''\n",
    "    # Decompose the time series into trend, seasonal, and residual components\n",
    "    decomposed_close <- decompose(close_ts)\n",
    "    \n",
    "    # Plot the decomposition\n",
    "    png(filename = \"decomposition_plot.png\", width = 800, height = 600)\n",
    "    plot(decomposed_close)\n",
    "    dev.off()\n",
    "''')\n",
    "\n",
    "# Display the decomposition plot\n",
    "display_r_plot(\"decomposition_plot.png\")\n",
    "ro.r('''\n",
    "\n",
    "# Plot the 'Close' price\n",
    "png(filename = \"Close_price.png\", width = 600, height = 600)\n",
    "\n",
    "# Plot the close prices directly without storing it in a variable\n",
    "plot(aapl_r_df$Date, aapl_r_df$log_Close, type = \"l\", col = \"blue\",\n",
    "     main = \"Log Close Price Over Time\", xlab = \"Date\", ylab = \"Close Price\")\n",
    "\n",
    "# Close the PNG device\n",
    "dev.off()\n",
    "\n",
    "# Check for stationarity using the Augmented Dickey-Fuller (ADF) test\n",
    "adf_test <- adf.test(aapl_r_df$log_Close)\n",
    "print(adf_test)\n",
    "''')\n",
    "\n",
    "# Display the plot with the corresponding filename\n",
    "display_r_plot(\"Close_price.png\")\n",
    "ro.r('''\n",
    "# Apply differencing to make the series stationary\n",
    "differenced_close <- diff(close_ts)\n",
    "\n",
    "# Plot the differenced series\n",
    "png(filename = \"differenced_close.png\", width = 800, height = 600)\n",
    "plot(differenced_close, main=\"Differenced Log Close Price Time Series\", ylab=\"Differenced Price\", xlab=\"Time\")\n",
    "dev.off()\n",
    "''')\n",
    "# Display the plot\n",
    "display_r_plot(\"differenced_close.png\")\n",
    "\n",
    "\n",
    "# Perform the ADF test on the differenced series\n",
    "ro.r('''\n",
    "adf_test_diff <- adf.test(differenced_close)\n",
    "print(adf_test_diff)\n",
    "''')\n",
    "# ACF plot example\n",
    "ro.r('''\n",
    "    # Open PNG device with a custom filename for ACF\n",
    "    png(filename = \"acf_plot.png\", width = 800, height = 600)\n",
    "    \n",
    "    # Create ACF plot\n",
    "    acf(aapl_r_df$Close, main=\"ACF of Log Close Prices\")\n",
    "    \n",
    "    # Close the PNG device\n",
    "    dev.off()\n",
    "''')\n",
    "\n",
    "# Display the ACF plot\n",
    "display_r_plot(\"acf_plot.png\")\n",
    "\n",
    "# PACF plot example\n",
    "ro.r('''\n",
    "    # Open PNG device with a custom filename for PACF\n",
    "    png(filename = \"pacf_plot.png\", width = 800, height = 600)\n",
    "    \n",
    "    # Create PACF plot\n",
    "    pacf(aapl_r_df$Close, main=\"PACF of Log Close Prices\")\n",
    "    \n",
    "    # Close the PNG device\n",
    "    dev.off()\n",
    "''')\n",
    "\n",
    "# Display the PACF plot\n",
    "display_r_plot(\"pacf_plot.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a46403",
   "metadata": {},
   "source": [
    "\n",
    "Our ACF is slowly decreasing, while the PACF cuts off at lag 2, indicating the presence of an autoregressive (AR) process. We have also differenced the data once to achieve stationarity, which suggests an integration order of 1. Based on the structure of the ACF, where lag 1 appears almost 1, an additional moving average (MA) component might also be necessary to capture the full dynamics of the data.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadb45aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ro.r(''' # ARIMA testing \n",
    "# Set up ranges for AR and MA components\n",
    "p_values <- c(1, 2)  # Possible values for AR (from PACF analysis)\n",
    "q_values <- c(0, 1, 2, 3)  # Possible values for MA (from ACF analysis)\n",
    "d <- 1  # The differencing order has already been determined\n",
    "\n",
    "# Initialize variables to store the best model and criteria\n",
    "best_aic <- Inf\n",
    "best_bic <- Inf\n",
    "best_model_aic <- NULL\n",
    "best_model_bic <- NULL\n",
    "\n",
    "# Loop over p and q values\n",
    "for (p in p_values) {\n",
    "  for (q in q_values) {\n",
    "    # Try fitting the ARIMA model with current p, d, q\n",
    "    arima_model <- tryCatch({arima(differenced_close, order = c(p, d, q))}, error = function(e) NULL)\n",
    "    \n",
    "    # Check if the model fitting was successful\n",
    "    if (!is.null(arima_model)) {\n",
    "      # Calculate AIC and BIC for the model\n",
    "      current_aic <- AIC(arima_model)\n",
    "      current_bic <- BIC(arima_model)\n",
    "      \n",
    "      # Update the best model based on AIC\n",
    "      if (current_aic < best_aic) {\n",
    "        best_aic <- current_aic\n",
    "        best_model_aic <- arima_model\n",
    "      }\n",
    "      \n",
    "      # Update the best model based on BIC\n",
    "      if (current_bic < best_bic) {\n",
    "        best_bic <- current_bic\n",
    "        best_model_bic <- arima_model\n",
    "      }\n",
    "      \n",
    "      # Print the result for this model\n",
    "      cat(\"ARIMA(\", p, \",\", d, \",\", q, \"): AIC =\", current_aic, \", BIC =\", current_bic, \"\\n\")\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "# Print the best models\n",
    "cat(\"\\nBest model based on AIC:\\n\")\n",
    "print(best_model_aic)\n",
    "cat(\"\\nBest model based on BIC:\\n\")\n",
    "print(best_model_bic)\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7d2f03",
   "metadata": {},
   "source": [
    "# Time Series Analysis (TSA) of AAPL Close Prices\n",
    "\n",
    "## Summary of TSA Process\n",
    "\n",
    "I conducted time series analysis (TSA) on the **AAPL Close prices**, focusing on selecting the best-fitting **ARIMA** model. My main goal was to model the temporal dependencies in the differenced and log-transformed series and identify the optimal model based on **AIC** and **BIC** criteria.\n",
    "\n",
    "### Preprocessing Steps\n",
    "\n",
    "1. **Log Transformation**: \n",
    "   - I log-transformed the **Close** prices to stabilize variance and improve normality in the time series data.\n",
    "   \n",
    "2. **Differencing**:\n",
    "   - To achieve stationarity, I differenced the log-transformed series once, which was confirmed by the **Augmented Dickey-Fuller (ADF) test**.\n",
    "   - The test indicated that the differenced series was stationary, with a **p-value of 0.01**.\n",
    "\n",
    "### ACF and PACF Analysis\n",
    "\n",
    "- **ACF**: The slow decay in the ACF plot suggested the need for a **moving average (MA)** component. Lag 1 showed a strong correlation close to 1, supporting an **MA(1)** process.\n",
    "- **PACF**: The sharp cut-off in the PACF after lag 2 indicated that an **AR(1)** or **AR(2)** process could be appropriate.\n",
    "\n",
    "### ARIMA Model Selection\n",
    "\n",
    "Based on the ACF and PACF plots, I tested several **ARIMA(p, d, q)** models with different AR (p) and MA (q) components. The differencing (d) was fixed at 1.\n",
    "\n",
    "#### ARIMA Model Results:\n",
    "\n",
    "| **ARIMA Model (p, d, q)** | **AIC**      | **BIC**      |\n",
    "|---------------------------|--------------|--------------|\n",
    "| ARIMA(1, 1, 0)            | -16903.04    | -16890.71    |\n",
    "| ARIMA(1, 1, 1)            | **-18368.09**| **-18349.59**|\n",
    "| ARIMA(1, 1, 2)            | -18366.17    | -18341.51    |\n",
    "| ARIMA(1, 1, 3)            | -18367.07    | -18336.24    |\n",
    "| ARIMA(2, 1, 0)            | -17295.14    | -17276.64    |\n",
    "| ARIMA(2, 1, 1)            | -18366.09    | -18341.43    |\n",
    "| ARIMA(2, 1, 2)            | -18364.15    | -18333.32    |\n",
    "| ARIMA(2, 1, 3)            | -18363.13    | -18326.14    |\n",
    "\n",
    "### Best ARIMA Model\n",
    "\n",
    "- **Best Model based on AIC**: **ARIMA(1, 1, 1)** with AIC = **-18368.09**.\n",
    "- **Best Model based on BIC**: **ARIMA(1, 1, 1)** with BIC = **-18349.59**.\n",
    "\n",
    "### ARIMA(1, 1, 1) Model Coefficients:\n",
    "\n",
    "- The **AR(1)** coefficient is **-0.0431** with a standard error of **0.0168**.\n",
    "- The **MA(1)** coefficient is **-1.0000** with a standard error of **0.0012**.\n",
    "- The **sigma^2** (error variance) is estimated as **0.0003154**.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "- The **ARIMA(1, 1, 1)** model was identified as the best fit for the differenced log-transformed data based on both AIC and BIC.\n",
    "- This model includes both an autoregressive (AR) and a moving average (MA) component, effectively capturing the underlying time dependencies in the data.\n",
    "- The low error variance and strong fit to the data make this model suitable for further time-series forecasting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4110c2a3",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "1. **Residual Diagnostics**: I will perform residual checks to ensure no remaining autocorrelation.\n",
    "2. **Advanced Models**: I will consider using **ARIMAX**, **GARCH**, or **EGARCH** to capture additional patterns, such as volatility clustering.\n",
    "3. **Forecasting**: I plan to use the selected ARIMA model to forecast future stock prices and assess its performance.\n",
    "\n",
    "This analysis provides a thorough statistical basis for the model selection, ensuring I choose the most appropriate time-series model for predictive purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6b0704",
   "metadata": {},
   "outputs": [],
   "source": [
    "ro.r(''' # Extract residuals from the best ARIMA model\n",
    "\n",
    "residuals_arima <- residuals(best_model_aic)\n",
    "\n",
    "# Plot residuals\n",
    "png(filename = \"residuals_plot.png\", width = 800, height = 600)\n",
    "plot(residuals_arima, main=\"Residuals from ARIMA(1, 1, 1)\", ylab=\"Residuals\", xlab=\"Time\")\n",
    "dev.off()\n",
    "''')\n",
    "display_r_plot(\"residuals_plot.png\")\n",
    "\n",
    "ro.r('''\n",
    "# Check for autocorrelation in the residuals using ACF\n",
    "png(filename = \"residuals_acf.png\", width = 800, height = 600)\n",
    "acf(residuals_arima, main=\"ACF of Residuals from ARIMA(1, 1, 1)\")\n",
    "dev.off()\n",
    "''')\n",
    "display_r_plot(\"residuals_acf.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a13cb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "ro.r(''' # Perform Ljung-Box test to check for any remaining autocorrelation\n",
    "ljung_box_test <- Box.test(residuals_arima, lag=10, type=\"Ljung-Box\")\n",
    "print(ljung_box_test)\n",
    "     ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd5c929",
   "metadata": {},
   "outputs": [],
   "source": [
    "ro.r(''' # ARIMA residuals\n",
    "arima_211 <- arima(differenced_close, order =c(2,1,1))\n",
    "residuals_arima_211 <- residuals(arima_211)\n",
    "     \n",
    "# Plot residuals from ARIMA(2, 1, 1)\n",
    "png(filename = \"residuals_arima_211.png\", width = 800, height = 600)\n",
    "plot(residuals_arima_211, main=\"Residuals from ARIMA(2, 1, 1)\", ylab=\"Residuals\", xlab=\"Time\")\n",
    "dev.off()\n",
    "''')\n",
    "display_r_plot(\"residuals_arima_211.png\")\n",
    "\n",
    "# Check the ACF of the residuals\n",
    "ro.r('''\n",
    "png(filename = \"acf_residuals_arima_211.png\", width = 800, height = 600)\n",
    "acf(residuals_arima_211, main=\"ACF of Residuals from ARIMA(2, 1, 1)\")\n",
    "dev.off()\n",
    "''')\n",
    "display_r_plot(\"acf_residuals_arima_211.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626b6efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ro.r(''' # Perform the Ljung-Box test to check for remaining autocorrelation\n",
    "ljung_box_test_211 <- Box.test(residuals_arima_211, lag=10, type=\"Ljung-Box\")\n",
    "print(ljung_box_test_211)\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d753a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ro.r(''' # ARIMA summary \n",
    "    # Print ARIMA(1,1,1) summary \n",
    "    print(summary(best_model_aic)) \n",
    "    # Print ARIMA(2,1,1) summary to see if change solved issues\n",
    "    print(summary(arima_211)) \n",
    "     ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b3551a",
   "metadata": {},
   "source": [
    "### ARIMA Model Selection and Analysis\n",
    "\n",
    "In the initial stages of Time Series Analysis, I explored various ARIMA models to identify the best fit for the data using AIC and BIC criteria. Both criteria converged on the same ARIMA model: ARIMA(1, 1, 1). **Since AIC and BIC agreed on the best model, this suggests that further exploration of ARIMA models (e.g., adding more AR or MA terms) would likely lead to overfitting without significant improvement in model accuracy.**\n",
    "\n",
    "**Upon analyzing the residuals from the selected ARIMA(1, 1, 1) model, it became clear that some autocorrelation remains, indicating that the model did not fully capture all the dependencies in the data. Despite this, adding additional AR or MA terms would not necessarily address these issues, as the model selection process already chose the best possible model based on statistical criteria.**\n",
    "\n",
    "\n",
    "\n",
    "Given that the series has already been differenced to achieve stationarity, there is no need for further differencing. \n",
    "\n",
    "### ARIMAX Model \n",
    "\n",
    "**ARIMAX (AutoRegressive Integrated Moving Average with Exogenous Variables) extends ARIMA by incorporating external predictors.** It is typically used when external factors significantly influence the target variable beyond autoregressive and moving average components.\n",
    "\n",
    "\n",
    "**Purpose**: Introduced ARIMAX to assess whether adding external variables (filtered via VIF to avoid collinearity issues with computations) could enhance the model’s performance by addressing volatility or autocorrelation in the differenced close data. Given that ARIMA(1,1,1) residuals displayed volatility spikes, the aim is to see if exogenous variables could help. \n",
    "\n",
    "Thus, I will proceed by exploring more advanced models to capture the remaining patterns in the data, rather than trying additional ARIMA variations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0903086d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ro.r(''' # Ensure differenced_close_ts and x_arimax cover the same time period\n",
    "\n",
    "# Slice differenced_close_ts to match training data (assuming it's time-series)\n",
    "     \n",
    "differenced_close_train <- differenced_close[1:nrow(transformed_df)]  # Adjust to fit training set size\n",
    "\n",
    "# Create the x_arimax matrix using only the predictors filtered from VIF analysis (without intercept)\n",
    "# Variables left after VIF filtering were: log_Volume, cube_volume_adi, cube_volume_obv, and volume_vpt\n",
    "# Calculate correlation matrix for the selected variables\n",
    "cor_matrix <- cor(transformed_df[, c(\"log_Volume\", \"cube_volume_obv\", \"volume_vpt\", \"cube_volume_adi\", 'cube_momentum_ao')])\n",
    "\n",
    "# Print the correlation matrix\n",
    "print(cor_matrix)\n",
    "\n",
    "# Given the correlation matrix, I tried log_Volume + cube_momentum_ao, and compared it to just log_Volume and cube_momentum_ao did not help, so I will only use ARIMAX with log_Volume \n",
    "x_arimax <- model.matrix(log_Close ~ log_Volume - 1, transformed_df)\n",
    "\n",
    "# Fit the ARIMAX model with the selected predictors (exclude intercept)\n",
    "arimax_model <- arima(differenced_close_train, order = c(1, 1, 1), xreg = x_arimax)\n",
    "\n",
    "# Summarize the ARIMAX model\n",
    "print(summary(arimax_model))\n",
    "\n",
    "# Extract residuals from the ARIMAX model\n",
    "residuals_arimax <- residuals(arimax_model)\n",
    "\n",
    "# Plot residuals from ARIMAX model\n",
    "png(filename = \"residuals_arimax_regularized.png\", width = 800, height = 600)\n",
    "plot(residuals_arimax, main=\"Residuals from ARIMAX(1, 1, 1)\", ylab=\"Residuals\", xlab=\"Time\")\n",
    "dev.off()\n",
    "''')\n",
    "\n",
    "# Display the residuals plot\n",
    "display_r_plot(\"residuals_arimax_regularized.png\")\n",
    "\n",
    "ro.r('''\n",
    "# Check the ACF of the residuals to check for any autocorrelation\n",
    "png(filename = \"acf_residuals_arimax_regularized.png\", width = 800, height = 600)\n",
    "acf(residuals_arimax, main=\"ACF of Residuals from ARIMAX(1, 1, 1)\")\n",
    "dev.off()\n",
    "''')\n",
    "# Display the ACF plot\n",
    "display_r_plot(\"acf_residuals_arimax_regularized.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf30a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "ro.r(''' # Perform Ljung-Box test to check for autocorrelation in residuals\n",
    "ljung_box_test_arimax <- Box.test(residuals_arimax, lag = 10, type = \"Ljung-Box\")\n",
    "print(ljung_box_test_arimax)\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e381bed",
   "metadata": {},
   "source": [
    "# ARIMAX Model Analysis and Conclusion\n",
    "\n",
    "### Why ARIMAX Did Not Improve Performance\n",
    "\n",
    "After establishing that the **ARIMA(1,1,1)** model adequately handled stationarity by ensuring a zero-mean and constant variance, I introduced the **ARIMAX** model to assess whether adding external variables (specifically VIF-filtered variables) could improve the model’s performance. However, the ARIMAX model did not lead to meaningful improvements for two primary reasons:\n",
    "\n",
    "1. **Singularity Problems**: When fitting ARIMAX with multiple predictors or collinear variables, the model encountered singularity issues. Singularity occurs when predictors are highly collinear, leading to unreliable coefficient estimates. In this case, the external variables added little new information beyond what the ARIMA model already captured.\n",
    "\n",
    "2. **Volatility Clustering**: The ARIMAX residuals still showed signs of autocorrelation, indicating unresolved volatility clustering. ARIMAX models are designed to capture mean shifts due to external variables, but they are not equipped to address time-varying volatility, which was evident in the data.\n",
    "\n",
    "### Conclusion and Next Steps\n",
    "\n",
    "**The ARIMAX model did not improve performance because the core issue lies in volatility clustering rather than mean shifts. Since ARIMAX is intended to account for changes in the mean from external shocks, it did not address the volatility in the residuals.** Thus, there is no justification for testing additional exogenous variables at this stage.\n",
    "\n",
    "Instead, I will shift focus to more advanced models designed for volatility, specifically **GARCH** and **EGARCH**, which are better suited for handling time-varying volatility. These models should resolve the remaining issues observed in the residuals.\n",
    "\n",
    "### Justification for Not Testing Additional Variables\n",
    "\n",
    "- **Volatility Clustering**: A **Box-Ljung Test** returned a p-value of **6.213e-07**, confirming that autocorrelation is still present in the residuals. This indicates that the primary issue is volatility, not mean shifts, making it inefficient to continue testing more external variables.\n",
    "  \n",
    "- **Singularity**: The singularity problems further support that the added predictors do not provide enough new information to justify additional testing with ARIMAX.\n",
    "\n",
    "*Note*: I will revisit ARIMAX using the **VIX** as an external variable, but for now, the focus remains on volatility modeling using **GARCH** and **EGARCH**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6734c114",
   "metadata": {},
   "source": [
    "## Transition to GARCH/EGARCH\n",
    "Given the presence of **volatility clustering**, I will now transition to **GARCH** and **EGARCH** models. These models are designed to capture **time-varying volatility**, which ARIMA and ARIMAX failed to address. Financial data often exhibit clusters of volatility that cannot be explained by shifts in the mean alone, making these models particularly suitable for this analysis.\n",
    "\n",
    "### Why GARCH and EGARCH?\n",
    "\n",
    "The **GARCH(1,1)** model captures conditional volatility, while the **EGARCH(1,1)** model accounts for **asymmetric effects**, where negative shocks have a larger impact on volatility. These features are essential for modeling financial market behaviors where large price swings tend to cluster.\n",
    "\n",
    "### Approaches to Handling Mean and Variance\n",
    "\n",
    "1. **GARCH(1,1) without a mean model**: The mean and variance are handled separately, with ARIMA addressing the mean structure and GARCH modeling the volatility.\n",
    "2. **GARCH(1,1) with ARMA(1,1)**: This integrates both the mean and variance components into one model, capturing both autocorrelation and volatility within a unified framework.\n",
    "\n",
    "By testing these models and their variations (with Normal and Student-t distributions), I aim to identify the most effective model for capturing both mean and volatility dynamics in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab9b50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ro.r(''' # GARCH modeling \n",
    "# Step 1: Define GARCH(1,1) with no mean model specification\n",
    "garch_spec <- ugarchspec(\n",
    "  variance.model = list(model = \"sGARCH\", garchOrder = c(1, 1)),\n",
    "  mean.model = list(armaOrder = c(0, 0), include.mean = FALSE), # No ARMA mean model since ARIMAX handled it\n",
    "  distribution.model = \"norm\"\n",
    ")\n",
    "\n",
    "# Step 2: Fit the GARCH(1,1) model using the differenced log close prices (training data)\n",
    "garch_fit <- ugarchfit(spec = garch_spec, data = differenced_close_train)\n",
    "\n",
    "# Step 3: Print the summary of the GARCH(1,1) model\n",
    "print(garch_fit)\n",
    "\n",
    "# Step 4: Extract residuals for diagnostics\n",
    "garch_residuals <- residuals(garch_fit, standardize = TRUE)\n",
    "\n",
    "# Convert residuals to a time series object\n",
    "garch_residuals_ts <- ts(garch_residuals, start = c(2009, 1), frequency = 252)  # Adjust the start date as needed\n",
    "\n",
    "# Step 5: Plot GARCH residuals with the correct time index\n",
    "png(filename = \"garch_residuals_fixed.png\", width = 800, height = 600)\n",
    "plot(garch_residuals_ts, main=\"Residuals from GARCH(1,1)\", ylab=\"Standardized Residuals\", xlab=\"Time\")\n",
    "dev.off()\n",
    "\n",
    "# Step 6: ACF of GARCH residuals\n",
    "png(filename = \"garch_acf_residuals_fixed.png\", width = 800, height = 600)\n",
    "acf(garch_residuals_ts, main=\"ACF of GARCH(1,1) Residuals\")\n",
    "dev.off()\n",
    "''')\n",
    "# Display the residuals plot\n",
    "display_r_plot(\"garch_residuals_fixed.png\")\n",
    "\n",
    "# Display the ACF plot\n",
    "display_r_plot(\"garch_acf_residuals_fixed.png\")\n",
    "\n",
    "ro.r('''\n",
    "# Step 1: Define GARCH(1,1) model with ARMA(1,1) mean model specification\n",
    "garch_spec_arma <- ugarchspec(\n",
    "  variance.model = list(model = \"sGARCH\", garchOrder = c(1, 1)),\n",
    "  mean.model = list(armaOrder = c(1, 1), include.mean = TRUE),  # ARMA(1,1) + GARCH(1,1)\n",
    "  distribution.model = \"norm\"\n",
    ")\n",
    "\n",
    "# Step 2: Fit ARMA(1,1) + GARCH(1,1) model using the differenced data, so it is technically ARIMA(1,1,1)\n",
    "garch_fit_arma <- ugarchfit(spec = garch_spec_arma, data = differenced_close_train)\n",
    "\n",
    "# Step 3: Print the summary of the ARMA(1,1) + GARCH(1,1) model\n",
    "print(garch_fit_arma)\n",
    "\n",
    "''')\n",
    "ro.r('''\n",
    "# Step 4: Extract residuals for diagnostics\n",
    "arma_garch_residuals <- residuals(garch_fit_arma, standardize = TRUE)\n",
    "\n",
    "# Convert residuals to a time series object (adjust dates as needed)\n",
    "arma_garch_residuals_ts <- ts(arma_garch_residuals, start = c(2009, 1), frequency = 252)  # Adjust the start date if necessary\n",
    "\n",
    "# Step 5: Plot ARMA(1,1) + GARCH(1,1) residuals\n",
    "png(filename = \"arma_garch_residuals.png\", width = 800, height = 600)\n",
    "plot(arma_garch_residuals_ts, main=\"Residuals from ARMA(1,1) + GARCH(1,1)\", ylab=\"Standardized Residuals\", xlab=\"Time\")\n",
    "dev.off()\n",
    "''')\n",
    "display_r_plot('arma_garch_residuals.png')\n",
    "\n",
    "\n",
    "ro.r('''\n",
    "# Step 6: ACF of ARMA(1,1) + GARCH(1,1) residuals\n",
    "png(filename = \"arma_garch_acf_residuals.png\", width = 800, height = 600)\n",
    "acf(arma_garch_residuals_ts, main=\"ACF of ARMA(1,1) + GARCH(1,1) Residuals\")\n",
    "dev.off()\n",
    "\n",
    "''')\n",
    "display_r_plot('arma_garch_acf_residuals.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b2597a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ro.r(''' # Autocorrelation significance test \n",
    "# Perform Ljung-Box test to check for autocorrelation in GARCH(1,1) residuals\n",
    "ljung_box_test_garch <- Box.test(garch_residuals, lag = 10, type = \"Ljung-Box\")\n",
    "print(ljung_box_test_garch)\n",
    "''')\n",
    "\n",
    "ro.r('''\n",
    "# Perform Ljung-Box test to check for autocorrelation in ARMA(1,1) + GARCH(1,1) residuals\n",
    "ljung_box_test_arma_garch <- Box.test(arma_garch_residuals, lag = 10, type = \"Ljung-Box\")\n",
    "print(ljung_box_test_arma_garch)\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cd87c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ro.r(''' # Normal vs std distribution \n",
    "# Model 1: GARCH(1,1) + ARMA(1,1) with normal distribution\n",
    "garch_spec_normal <- ugarchspec(\n",
    "  variance.model = list(model = \"sGARCH\", garchOrder = c(1, 1)),\n",
    "  mean.model = list(armaOrder = c(1, 1), include.mean = TRUE),  # ARMA(1,1)\n",
    "  distribution.model = \"norm\"\n",
    ")\n",
    "\n",
    "garch_fit_normal <- ugarchfit(spec = garch_spec_normal, data = differenced_close_train)\n",
    "aic_normal <- infocriteria(garch_fit_normal)[\"Akaike\",]\n",
    "bic_normal <- infocriteria(garch_fit_normal)[\"Bayes\",]\n",
    "print(paste(\"AIC (Normal):\", aic_normal))\n",
    "print(paste(\"BIC (Normal):\", bic_normal))\n",
    "     \n",
    "\n",
    "# Model 2: GARCH(1,1) + ARMA(1,1) with Student-t distribution\n",
    "garch_spec_student <- ugarchspec(\n",
    "  variance.model = list(model = \"sGARCH\", garchOrder = c(1, 1)),\n",
    "  mean.model = list(armaOrder = c(1, 1), include.mean = TRUE),  # ARMA(1,1)\n",
    "  distribution.model = \"std\"  # Student-t distribution\n",
    ")\n",
    "\n",
    "garch_fit_student <- ugarchfit(spec = garch_spec_student, data = differenced_close_train)\n",
    "aic_student <- infocriteria(garch_fit_student)[\"Akaike\",]\n",
    "bic_student <- infocriteria(garch_fit_student)[\"Bayes\",]\n",
    "print(paste(\"AIC (Student-t):\", aic_student))\n",
    "print(paste(\"BIC (Student-t):\", bic_student))\n",
    "     \n",
    "# Model 3: GARCH(1,1) with no mean model (Normal distribution)\n",
    "garch_spec_no_mean_normal <- ugarchspec(\n",
    "  variance.model = list(model = \"sGARCH\", garchOrder = c(1, 1)),\n",
    "  mean.model = list(armaOrder = c(0, 0), include.mean = FALSE),  # No mean model\n",
    "  distribution.model = \"norm\"\n",
    ")\n",
    "\n",
    "garch_fit_no_mean_normal <- ugarchfit(spec = garch_spec_no_mean_normal, data = differenced_close_train)\n",
    "aic_no_mean_normal <- infocriteria(garch_fit_no_mean_normal)[\"Akaike\",]\n",
    "bic_no_mean_normal <- infocriteria(garch_fit_no_mean_normal)[\"Bayes\",]\n",
    "print(paste(\"AIC (No Mean, Normal):\", aic_no_mean_normal))\n",
    "print(paste(\"BIC (No Mean, Normal):\", bic_no_mean_normal))\n",
    "\n",
    "# Model 4: GARCH(1,1) with no mean model (Student-t distribution)\n",
    "garch_spec_no_mean_student <- ugarchspec(\n",
    "  variance.model = list(model = \"sGARCH\", garchOrder = c(1, 1)),\n",
    "  mean.model = list(armaOrder = c(0, 0), include.mean = FALSE),  # No mean model\n",
    "  distribution.model = \"std\"\n",
    ")\n",
    "\n",
    "garch_fit_no_mean_student <- ugarchfit(spec = garch_spec_no_mean_student, data = differenced_close_train)\n",
    "aic_no_mean_student <- infocriteria(garch_fit_no_mean_student)[\"Akaike\",]\n",
    "bic_no_mean_student <- infocriteria(garch_fit_no_mean_student)[\"Bayes\",]\n",
    "print(paste(\"AIC (No Mean, Student-t):\", aic_no_mean_student))\n",
    "print(paste(\"BIC (No Mean, Student-t):\", bic_no_mean_student))\n",
    "\n",
    "     \n",
    "# Model 5: EGARCH(1,1) with Normal distribution\n",
    "egarch_spec_normal <- ugarchspec(\n",
    "  variance.model = list(model = \"eGARCH\", garchOrder = c(1, 1)),\n",
    "  mean.model = list(armaOrder = c(1, 1), include.mean = TRUE),  # ARMA(1,1)\n",
    "  distribution.model = \"norm\"\n",
    ")\n",
    "\n",
    "egarch_fit_normal <- ugarchfit(spec = egarch_spec_normal, data = differenced_close_train)\n",
    "aic_egarch_normal <- infocriteria(egarch_fit_normal)[\"Akaike\",]\n",
    "bic_egarch_normal <- infocriteria(egarch_fit_normal)[\"Bayes\",]\n",
    "print(paste(\"AIC (EGARCH Normal):\", aic_egarch_normal))\n",
    "print(paste(\"BIC (EGARCH Normal):\", bic_egarch_normal))\n",
    "\n",
    "# Model 6: EGARCH(1,1) with Student-t distribution\n",
    "egarch_spec_student <- ugarchspec(\n",
    "  variance.model = list(model = \"eGARCH\", garchOrder = c(1, 1)),\n",
    "  mean.model = list(armaOrder = c(1, 1), include.mean = TRUE),  # ARMA(1,1)\n",
    "  distribution.model = \"std\"\n",
    ")\n",
    "\n",
    "egarch_fit_student <- ugarchfit(spec = egarch_spec_student, data = differenced_close_train)\n",
    "aic_egarch_student <- infocriteria(egarch_fit_student)[\"Akaike\",]\n",
    "bic_egarch_student <- infocriteria(egarch_fit_student)[\"Bayes\",]\n",
    "print(paste(\"AIC (EGARCH Student-t):\", aic_egarch_student))\n",
    "print(paste(\"BIC (EGARCH Student-t):\", bic_egarch_student))\n",
    "     ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b543ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "ro.r(''' # Extract residuals for each model \n",
    "garch_residuals_normal <- residuals(garch_fit_normal, standardize = TRUE)\n",
    "garch_residuals_student <- residuals(garch_fit_student, standardize = TRUE)\n",
    "garch_residuals_no_mean_student <- residuals(garch_fit_no_mean_student, standardize = TRUE)\n",
    "garch_residuals_no_mean_normal <- residuals(garch_fit_no_mean_normal, standardize = TRUE)\n",
    "garch_residuals_student <- residuals(garch_fit_student, standardize = TRUE)\n",
    "egarch_residuals_normal <- residuals(egarch_fit_normal, standardize = TRUE)\n",
    "egarch_residuals_student <- residuals(egarch_fit_student, standardize = TRUE)\n",
    "\n",
    "\n",
    "# Perform Ljung-Box test for all models\n",
    "     \n",
    "ljung_box_garch_normal <- Box.test(garch_residuals_normal, lag = 10, type = \"Ljung-Box\")\n",
    "print(ljung_box_garch_normal)\n",
    "\n",
    "ljung_box_garch_student <- Box.test(garch_residuals_student, lag = 10, type = \"Ljung-Box\")\n",
    "print(ljung_box_garch_student)\n",
    "\n",
    "ljung_box_garch_normal <- Box.test(garch_residuals_no_mean_normal, lag = 10, type = \"Ljung-Box\")\n",
    "print(ljung_box_garch_normal)\n",
    "\n",
    "ljung_box_garch_student <- Box.test(garch_residuals_no_mean_student, lag = 10, type = \"Ljung-Box\")\n",
    "print(ljung_box_garch_student)     \n",
    "\n",
    "ljung_box_egarch_normal <- Box.test(egarch_residuals_normal, lag = 10, type = \"Ljung-Box\")\n",
    "print(ljung_box_egarch_normal)\n",
    "\n",
    "ljung_box_egarch_student <- Box.test(egarch_residuals_student, lag = 10, type = \"Ljung-Box\")\n",
    "print(ljung_box_egarch_student)\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db911119",
   "metadata": {},
   "source": [
    "## Advanced Time Series Analysis (TSA): GARCH and EGARCH Models\n",
    "\n",
    "### Summary of GARCH/EGARCH Process\n",
    "\n",
    "After conducting ARIMA and ARIMAX modeling, I found that **volatility clustering** remained a core issue that those models could not address. Thus, I moved forward with **GARCH** and **EGARCH** models to better handle time-varying volatility in the data.\n",
    "\n",
    "### GARCH and EGARCH Models Tested\n",
    "\n",
    "To fully capture both mean and volatility, I tested several GARCH and EGARCH variants. Some models incorporated an **ARMA(1,1)** mean model, while others did not.\n",
    "\n",
    "- **Models with ARMA(1,1)**: These models use an **ARIMA(1,1,1)** framework to handle autocorrelation and differencing of the series. \n",
    "- **Models without ARMA(1,1)**: These models are purely volatility-focused, meaning they do not include a mean structure like ARIMA.\n",
    "\n",
    "Here are the six models tested:\n",
    "\n",
    "1. **GARCH(1,1) with Normal Distribution**: Includes **ARMA(1,1)**.\n",
    "2. **GARCH(1,1) with Student-t Distribution**: Includes **ARMA(1,1)**.\n",
    "3. **GARCH(1,1) with No Mean Model (Normal Distribution)**: No ARMA component.\n",
    "4. **GARCH(1,1) with No Mean Model (Student-t Distribution)**: No ARMA component.\n",
    "5. **EGARCH(1,1) with Normal Distribution**: Includes **ARMA(1,1)**.\n",
    "6. **EGARCH(1,1) with Student-t Distribution**: Includes **ARMA(1,1)**.\n",
    "\n",
    "### Model Selection Criteria: AIC and BIC\n",
    "\n",
    "| **Model**                               | **AIC**             | **BIC**             |\n",
    "|-----------------------------------------|---------------------|---------------------|\n",
    "| GARCH(1,1) with Normal                  | -5.3826             | -5.3697             |\n",
    "| GARCH(1,1) with Student-t               | -5.4908             | -5.4758             |\n",
    "| GARCH(1,1) No Mean, Normal              | -5.3709             | -5.3645             |\n",
    "| GARCH(1,1) No Mean, Student-t           | -5.4792             | -5.4707             |\n",
    "| EGARCH(1,1) with Normal                 | -5.4152             | -5.4002             |\n",
    "| **EGARCH(1,1) with Student-t**          | **-5.5138**         | **-5.4967**         |\n",
    "\n",
    "#### Best Model Based on AIC and BIC:\n",
    "- **Best Model based on AIC**: **EGARCH(1,1) with Student-t distribution**.\n",
    "- **Best Model based on BIC**: **EGARCH(1,1) with Student-t distribution**.\n",
    "\n",
    "### Box-Ljung Test Results for Residual Autocorrelation\n",
    "\n",
    "| **Model**                               | **Box-Ljung Test (p-value)** |\n",
    "|-----------------------------------------|-------------------------------|\n",
    "| GARCH(1,1) with Normal                  | 0.1418                        |\n",
    "| GARCH(1,1) with Student-t               | 0.1473                        |\n",
    "| GARCH(1,1) No Mean, Normal              | 0.1432                        |\n",
    "| GARCH(1,1) No Mean, Student-t           | 0.1522                        |\n",
    "| EGARCH(1,1) with Normal                 | 0.131                         |\n",
    "| **EGARCH(1,1) with Student-t**          | **0.1572**                    |\n",
    "\n",
    "#### Interpretation:\n",
    "- All models have **p-values > 0.05**, suggesting to **fail to reject the null hypothesis** that there is no autocorrelation in the residuals. This indicates that all models adequately captured the volatility in the data.\n",
    "- **EGARCH(1,1) with Student-t distribution** produced the best AIC, BIC, and passed the Box-Ljung test, making it the most appropriate model for this dataset.\n",
    "\n",
    "### Why EGARCH(1,1) with Student-t Distribution is the Best Model\n",
    "\n",
    "- **EGARCH (Exponential GARCH)**: This model can capture **asymmetric effects**, where negative shocks tend to increase volatility more than positive shocks.\n",
    "  \n",
    "- **Student-t Distribution**: Its heavy tails better capture large movements in stock prices than the normal distribution.\n",
    "\n",
    "- **ARIMA(1,1,1) + EGARCH(1,1)**: By incorporating the **ARIMA(1,1,1)** model, I addressed the autocorrelation and stationarity in the mean, while **EGARCH(1,1)** captured the time-varying volatility.\n",
    "\n",
    "- **Conclusion**: The **EGARCH(1,1) with Student-t distribution** model provides the best fit based on statistical criteria (AIC, BIC) and residual diagnostics, effectively handling both the mean structure and volatility clustering in the AAPL data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0e74b2",
   "metadata": {},
   "source": [
    "## Integration of EGARCH Model into the Predictive Framework\n",
    "\n",
    "Having identified the **EGARCH(1,1) with Student-t distribution** model as the best-performing time-series model, I am now incorporating its volatility predictions into the regression framework. The goal is to enhance the performance of both robust and regularized regression methods by accounting for the time-varying volatility captured by EGARCH. This step will allow the model to handle not just the mean structure, but also the heteroscedasticity inherent in the data, improving predictive accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d283533e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ro.r(''' # Integrate EGARCH into regression models\n",
    "set.seed(123)\n",
    "# Step 1: Generate Volatility Predictions from EGARCH(1,1) with Student-t distribution\n",
    "egarch_volatility <- sigma(egarch_fit_student)  # Extract the volatility from EGARCH\n",
    "\n",
    "# Add the volatility predictions as a new column to the training dataset\n",
    "transformed_df$egarch_volatility <- egarch_volatility\n",
    "\n",
    "# Step 2: Update Robust Regression Models with EGARCH Volatility\n",
    "# Fit robust regression model\n",
    "robust_model_egarch <- rlm(log_Close ~ . + egarch_volatility, data = transformed_df)\n",
    "\n",
    "# Predict on the test data with added volatility feature\n",
    "transformed_test_df$egarch_volatility <- sigma(egarch_fit_student)[1:nrow(transformed_test_df)]\n",
    "robust_preds_egarch <- predict(robust_model_egarch, newdata = transformed_test_df)\n",
    "\n",
    "# Calculate MSPE for robust model with EGARCH volatility\n",
    "robust_mspe_egarch <- mean((test_y - robust_preds_egarch)^2)\n",
    "cat(\"EGARCH-Enhanced Huber Regression MSPE:\", robust_mspe_egarch, \"\\n\")\n",
    "\n",
    "# Step 3: Update QR and LTS Models with EGARCH Volatility\n",
    "\n",
    "# Fit Quantile Regression (QR) model with EGARCH volatility at tau = 0.5 (median)\n",
    "qr_model_egarch <- rq(log_Close ~ . + egarch_volatility, data = transformed_df, tau = 0.5)\n",
    "\n",
    "# Predict on the test data for QR model\n",
    "qr_preds_egarch <- predict(qr_model_egarch, newdata = transformed_test_df)\n",
    "\n",
    "# Calculate MSPE for QR model with EGARCH volatility\n",
    "qr_mspe_egarch <- mean((test_y - qr_preds_egarch)^2)\n",
    "cat(\"EGARCH-Enhanced Quantile Regression MSPE:\", qr_mspe_egarch, \"\\n\")\n",
    "\n",
    "\n",
    "# Fit Least Trimmed Squares (LTS) model with EGARCH volatility\n",
    "lts_model_egarch <- lqs(log_Close ~ . + egarch_volatility, data = transformed_df)\n",
    "\n",
    "# Predict on the test data for LTS model\n",
    "lts_preds_egarch <- predict(lts_model_egarch, newdata = transformed_test_df)\n",
    "\n",
    "# Calculate MSPE for LTS model with EGARCH volatility\n",
    "lts_mspe_egarch <- mean((test_y - lts_preds_egarch)^2)\n",
    "cat(\"EGARCH-Enhanced Least Trimmed Squares Regression MSPE:\", lts_mspe_egarch, \"\\n\")\n",
    "\n",
    "     \n",
    "# Step 3: Update Regularized Regression Models with EGARCH Volatility\n",
    "# Convert predictors and response using model.matrix including EGARCH volatility\n",
    "x_egarch <- model.matrix(log_Close ~ . + egarch_volatility, transformed_df)[, -1]  # Exclude intercept\n",
    "y <- transformed_df$log_Close  # Response variable\n",
    "\n",
    "# Fit Ridge, Lasso, and ElasticNet models with EGARCH volatility\n",
    "ridge_model_egarch <- cv.glmnet(x_egarch, y, alpha = 0)\n",
    "lasso_model_egarch <- cv.glmnet(x_egarch, y, alpha = 1)\n",
    "elasticnet_model_egarch <- cv.glmnet(x_egarch, y, alpha = 0.5)\n",
    "\n",
    "# Predict on test data\n",
    "x_test_egarch <- model.matrix(log_Close ~ . + egarch_volatility, transformed_test_df)[, -1]  # Exclude intercept\n",
    "ridge_preds_egarch <- predict(ridge_model_egarch, newx = x_test_egarch)\n",
    "lasso_preds_egarch <- predict(lasso_model_egarch, newx = x_test_egarch)\n",
    "elasticnet_preds_egarch <- predict(elasticnet_model_egarch, newx = x_test_egarch)\n",
    "\n",
    "# Calculate MSPE for each regularized regression model with EGARCH volatility\n",
    "ridge_mspe_egarch <- mean((test_y - ridge_preds_egarch)^2)\n",
    "lasso_mspe_egarch <- mean((test_y - lasso_preds_egarch)^2)\n",
    "elasticnet_mspe_egarch <- mean((test_y - elasticnet_preds_egarch)^2)\n",
    "\n",
    "cat(\"EGARCH-Enhanced Ridge Regression MSPE:\", ridge_mspe_egarch, \"\\n\")\n",
    "cat(\"EGARCH-Enhanced Lasso Regression MSPE:\", lasso_mspe_egarch, \"\\n\")\n",
    "cat(\"EGARCH-Enhanced ElasticNet Regression MSPE:\", elasticnet_mspe_egarch, \"\\n\")\n",
    "     \n",
    "# Step 4: Update VIF Model and Full Model to include EGARCH Volatility\n",
    "vif_model_egarch <- lm(log_Close ~ log_Volume + cube_volume_adi + cube_volume_obv + volume_vpt + cube_momentum_ao + egarch_volatility, data = transformed_df)\n",
    "full_model_egarch <- lm(log_Close ~ . + egarch_volatility, data = transformed_df)\n",
    "     \n",
    "transformed_test_df$egarch_volatility <- sigma(egarch_fit_student)[1:nrow(transformed_test_df)]\n",
    "vif_preds_egarch <- predict(vif_model_egarch, newdata = transformed_test_df)\n",
    "vif_mspe_egarch <- mean((test_y - vif_preds_egarch)^2)\n",
    "     \n",
    "full_preds_egarch <- predict(full_model_egarch, newdata = transformed_test_df)\n",
    "full_mspe_egarch <- mean((test_y - full_preds_egarch)^2)\n",
    "cat(\"EGARCH-Enhanced VIF Model MSPE:\", vif_mspe_egarch, \"\\n\")\n",
    "cat(\"EGARCH-Enhanced Full Model MSPE:\", full_mspe_egarch, \"\\n\")\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5159a569",
   "metadata": {},
   "outputs": [],
   "source": [
    "ro.r(''' # Helper function to apply smearing (bias correction) and calculate MSPE in original scale\n",
    "calculate_mspe_with_smearing <- function(log_preds, log_true) {\n",
    "    # Exponentiate the predictions and true values to get them on the original scale\n",
    "    pred_original <- exp(log_preds)\n",
    "    true_original <- exp(log_true)\n",
    "    \n",
    "    # Calculate residuals on the log scale\n",
    "    residuals_log <- log_true - log_preds\n",
    "    \n",
    "    # Bias correction using the smearing estimator\n",
    "    CF <- mean(exp(residuals_log))  # Correction factor for bias\n",
    "    adjusted_preds_original <- pred_original * CF\n",
    "    \n",
    "    # Calculate MSPE on the original scale\n",
    "    mspe_original <- mean((true_original - adjusted_preds_original)^2)\n",
    "    \n",
    "    return(mspe_original)\n",
    "}\n",
    "\n",
    "# Step 4: Apply smearing for all EGARCH-enhanced models\n",
    "\n",
    "# Huber Regression with EGARCH Volatility (Original Scale)\n",
    "robust_mspe_egarch_original <- calculate_mspe_with_smearing(robust_preds_egarch, test_y)\n",
    "cat(\"EGARCH-Enhanced Huber Regression MSPE (Original Scale):\", robust_mspe_egarch_original, \"\\n\")\n",
    "\n",
    "# Quantile Regression (QR) with EGARCH Volatility (Original Scale)\n",
    "qr_mspe_egarch_original <- calculate_mspe_with_smearing(qr_preds_egarch, test_y)\n",
    "cat(\"EGARCH-Enhanced Quantile Regression MSPE (Original Scale):\", qr_mspe_egarch_original, \"\\n\")\n",
    "\n",
    "# Least Trimmed Squares (LTS) with EGARCH Volatility (Original Scale)\n",
    "lts_mspe_egarch_original <- calculate_mspe_with_smearing(lts_preds_egarch, test_y)\n",
    "cat(\"EGARCH-Enhanced Least Trimmed Squares Regression MSPE (Original Scale):\", lts_mspe_egarch_original, \"\\n\")\n",
    "\n",
    "# Ridge Regression with EGARCH Volatility (Original Scale)\n",
    "ridge_mspe_egarch_original <- calculate_mspe_with_smearing(ridge_preds_egarch, test_y)\n",
    "cat(\"EGARCH-Enhanced Ridge Regression MSPE (Original Scale):\", ridge_mspe_egarch_original, \"\\n\")\n",
    "\n",
    "# Lasso Regression with EGARCH Volatility (Original Scale)\n",
    "lasso_mspe_egarch_original <- calculate_mspe_with_smearing(lasso_preds_egarch, test_y)\n",
    "cat(\"EGARCH-Enhanced Lasso Regression MSPE (Original Scale):\", lasso_mspe_egarch_original, \"\\n\")\n",
    "\n",
    "# ElasticNet Regression with EGARCH Volatility (Original Scale)\n",
    "elasticnet_mspe_egarch_original <- calculate_mspe_with_smearing(elasticnet_preds_egarch, test_y)\n",
    "cat(\"EGARCH-Enhanced ElasticNet Regression MSPE (Original Scale):\", elasticnet_mspe_egarch_original, \"\\n\")\n",
    "   \n",
    "\n",
    "vif_mspe_egarch_original <- calculate_mspe_with_smearing(vif_preds_egarch, test_y)\n",
    "cat(\"VIF Model with EGARCH Volatility and Smearing MSPE (Original Scale):\", vif_mspe_egarch_original, \"\\n\")\n",
    "\n",
    "# Step 6: Apply smearing bias correction for Full Model with EGARCH Volatility \n",
    "full_model_mspe_egarch_original <- calculate_mspe_with_smearing(full_preds_egarch, test_y)\n",
    "cat(\"Full Model with EGARCH Volatility and Smearing MSPE (Original Scale):\", full_model_mspe_egarch_original, \"\\n\")\n",
    "\n",
    "     ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f845b983",
   "metadata": {},
   "source": [
    "# Analysis of EGARCH-Enhanced Regression Methods\n",
    "\n",
    "## Overview\n",
    "\n",
    "This analysis applied robust regression methods (Quantile Regression, Huber Regression, and Least Trimmed Squares), regularized regression methods (Ridge, Lasso, and Elastic Net), and EGARCH-enhanced versions of these models. The objective was to assess whether incorporating EGARCH volatility into the models would improve predictive performance by addressing volatility and outliers in financial data.\n",
    "\n",
    "## Results\n",
    "\n",
    "### EGARCH-Enhanced Regression Models\n",
    "\n",
    "| **Model**                           | **MSPE (Log Scale)**   | **MSPE (Original Scale)** |\n",
    "|-------------------------------------|------------------------|---------------------------|\n",
    "| EGARCH-Enhanced Huber Regression    | 4.948813e-05           | 1.097161                  |\n",
    "| EGARCH-Enhanced Quantile Regression | 5.062403e-05           | 1.083868                  |\n",
    "| EGARCH-Enhanced Least Trimmed Squares Regression | 0.0001883546 | 4.45806                  |\n",
    "| EGARCH-Enhanced Ridge Regression    | 0.000681377            | 15.3242                   |\n",
    "| EGARCH-Enhanced Lasso Regression    | 0.002135133            | 4.930664                  |\n",
    "| EGARCH-Enhanced ElasticNet Regression | 0.0008286128         | 8.247073                  |\n",
    "| EGARCH-Enhanced VIF Model           | 0.1123632              | 4630.846                  |\n",
    "| EGARCH-Enhanced Full Model          | 6.048988e-05           | 1.335532                  |\n",
    "\n",
    "### Standard Robust and Regularized Regression Methods\n",
    "\n",
    "| **Model**                          | **MSPE (Log Scale)** | **MSPE (Original Scale)** |\n",
    "|------------------------------------|----------------------|---------------------------|\n",
    "| Quantile Regression                | 3.6021e-05           | 0.8046161                 |\n",
    "| Huber Regression                   | 3.789034e-05         | 0.8522115                 |\n",
    "| Penalized Quantile Regression      | 7.939872e-05         | 1.758258                  |\n",
    "| Least Trimmed Squares Regression   | 0.0007393806         | 15.15739                  |\n",
    "\n",
    "### Regularized Regression Methods\n",
    "\n",
    "| **Model**                  | **MSPE (Log Scale)** | **MSPE (Original Scale)** |\n",
    "|----------------------------|----------------------|---------------------------|\n",
    "| Ridge Regression            | 0.0006943541         | 16.37199                  |\n",
    "| Lasso Regression            | 0.002135133          | 4.930664                  |\n",
    "| Elastic Net Regression      | 0.002135133          | 4.930664                  |\n",
    "| VIF Model                   | 0.08338693           | 3076.539                  |\n",
    "| Full Model                  | 4.225889e-05         | 0.9543641                 |\n",
    "\n",
    "## Interpretation of Results\n",
    "\n",
    "### General Observations\n",
    "\n",
    "1. **Increased MSPE Across Most Models**: The EGARCH-enhanced models generally performed worse than their original versions, with higher MSPE on both the log and original scales. This can be attributed to the introduction of volatility features that added complexity to the models, which were not well-suited to handle the nonlinear nature of time-varying volatility.\n",
    "\n",
    "2. **Robust Methods with EGARCH**: Both **Quantile Regression** and **Huber Regression** showed minimal increases in MSPE after incorporating EGARCH volatility. The results indicate that these models, which are inherently robust to outliers, were not significantly affected by volatility adjustments. However, their predictive accuracy did not substantially improve either, suggesting that the EGARCH volatility did not provide substantial new information.\n",
    "\n",
    "3. **Improvement in Least Trimmed Squares (LTS)**: Interestingly, **Least Trimmed Squares Regression** showed improvement after introducing EGARCH volatility, reducing its MSPE from **15.15739 (Original Scale)** to **4.45806**. This improvement suggests that LTS, which minimizes the influence of extreme values, benefited from the EGARCH volatility's ability to account for large swings in the data, reducing the impact of outliers.\n",
    "\n",
    "### Regularized Regression Methods\n",
    "\n",
    "4. **Ridge Regression with EGARCH**: **Ridge Regression** showed a slight improvement after incorporating EGARCH, with the MSPE improving on the log scale from **0.0006943541** to **0.000681377**. However, the improvement was not as significant on the original scale, indicating that while Ridge Regression handled multicollinearity effectively, it did not fully capitalize on the volatility captured by EGARCH.\n",
    "\n",
    "5. **Lasso Regression**: **Lasso Regression** remained stable with no significant change after the EGARCH adjustment, maintaining an MSPE of **4.930664** on both scales. This shows that Lasso was largely unaffected by the volatility adjustment, as the sparsity enforced by the L1 penalty helped retain its predictive performance.\n",
    "\n",
    "6. **Elastic Net Regression Got Worse**: The **Elastic Net Regression** model's performance deteriorated with EGARCH volatility, seeing an increase in MSPE from **4.930664** to **8.247073**. This suggests that the combination of L1 and L2 penalties could not balance the added complexity from the volatility, leading to a degradation in prediction accuracy.\n",
    "\n",
    "7. **VIF Model and Full Model with EGARCH**: The **VIF Model** performed poorly even with EGARCH volatility, with an extremely high MSPE of **4630.846 (Original Scale)**. This likely resulted from the model’s inability to handle the complexity introduced by both multicollinearity and volatility, leading to poor predictions. The **Full Model**, though not as poor, also showed an increase in MSPE from **0.9543641 (Original Scale)** to **1.335532**, indicating a deterioration in performance when EGARCH volatility was introduced.\n",
    "\n",
    "## Mathematical and Theoretical Reasoning\n",
    "\n",
    "1. **Nonlinear Complexity**: EGARCH models are specifically designed to capture time-varying volatility, which is a non-linear characteristic of financial data. However, the regularized regression models (Ridge, Lasso, Elastic Net) are linear models, which means they struggle to incorporate and benefit from the volatility features introduced by EGARCH. This disconnect between the linear nature of the models and the nonlinear characteristics of the data likely led to increased MSPE.\n",
    "\n",
    "2. **Smearing Bias Correction**: The application of smearing bias correction further exposed the limitations of these models in handling volatility. The correction for bias due to log transformation revealed that the models were consistently underperforming when predicting the original (untransformed) values of stock prices.\n",
    "\n",
    "3. **Impact of EGARCH on Robust Methods**: While robust methods like Huber and Quantile Regression are designed to minimize the influence of extreme values, the introduction of EGARCH volatility did not provide significant new information to enhance their performance. The nature of these models already focuses on handling irregularities in the data, and thus the volatility adjustment did not substantially improve their predictions.\n",
    "\n",
    "4. **Improvement in LTS**: The significant improvement in **Least Trimmed Squares Regression** after incorporating EGARCH volatility suggests that volatility features helped reduce the model’s sensitivity to extreme outliers, making it more stable and reliable in handling the financial data's inherent volatility.\n",
    "\n",
    "## Conclusions\n",
    "\n",
    "1. **EGARCH Volatility Did Not Enhance Regularized Regression**: Despite the complexity introduced by EGARCH volatility, the regularized methods (Lasso, Elastic Net, and Ridge) did not benefit substantially, with only marginal improvements in Ridge Regression. The primary reason lies in the disconnect between the non-linear nature of volatility and the linear structure of regularized regression models.\n",
    "\n",
    "2. **Robust Methods Remain Strong Contenders**: Quantile Regression and Huber Regression maintained their strength even after introducing EGARCH volatility, but they did not show significant improvement. Least Trimmed Squares (LTS) saw the most notable improvement after EGARCH was introduced, highlighting its potential for handling volatility-driven data.\n",
    "\n",
    "3. **Practical Implications**: The results suggest that while EGARCH is highly effective for volatility modeling, its integration into linear regression models may not yield substantial benefits unless the model structure itself is adapted to handle non-linearities. Financial models that directly address volatility through methods such as LTS or Ridge Regression are more suited to capture the characteristics of the data when combined with volatility features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f241a65",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Practical Implications and Future Directions\n",
    "\n",
    "The analysis demonstrated that the integration of **EGARCH volatility** into the regression framework did not yield significant improvements in most regularized methods. Despite the ability of the **EGARCH** model to capture volatility clustering, the linear nature of **Ridge**, **Lasso**, and **Elastic Net** models seems to have struggled to leverage the non-linear volatility insights provided by the EGARCH model. \n",
    "\n",
    "However, there were **notable improvements** in **Ridge** and **Least Trimmed Squares** models, showcasing that **EGARCH-enhanced volatility** can enhance models that are more sensitive to outliers or multicollinearity. This insight suggests that future work should focus on models capable of handling non-linear relationships more effectively.\n",
    "\n",
    "---\n",
    "\n",
    "## In Progress / To Be Completed\n",
    "\n",
    "### Residual Diagnostics and Error Distribution Analysis (In Progress)\n",
    "\n",
    "- **Objective**: Evaluate the fit of residuals from all models to determine if further transformations or adjustments are needed.\n",
    "- **Focus**: Explore **ARCH** effects in residuals, and conduct **Q-Q plots** to assess normality and the fit of residuals.\n",
    "\n",
    "---\n",
    "\n",
    "### Non-Parametric and Machine Learning Methods (Next Step)\n",
    "\n",
    "- **Objective**: Given the limited success of linear regression methods, I plan to implement **non-parametric and machine learning models** like **Random Forests** and **Gradient Boosting Machines (GBM)** to capture the non-linear effects present in financial data.\n",
    "- **Reasoning**: Machine learning methods are better suited for capturing complex relationships and interactions between features that EGARCH volatility introduces but linear models fail to exploit fully.\n",
    "\n",
    "---\n",
    "\n",
    "### Ensemble Methods for Model Combination (Planned)\n",
    "\n",
    "- **Objective**: Combine the best performing models (e.g., **Quantile Regression** and **LTS**) with non-parametric methods to enhance predictive accuracy and stability.\n",
    "- **Focus**: Utilize techniques like **stacking** and **blending** to leverage the strengths of various models. This approach can create a more balanced predictive strategy that incorporates linear, non-linear, and volatility-based insights.\n",
    "\n",
    "---\n",
    "\n",
    "### Model Validation and Backtesting (In Progress)\n",
    "\n",
    "- **Objective**: Rigorously backtest the models using financial metrics such as **Sharpe Ratio** and **Value at Risk (VaR)** to validate their performance in real-world trading conditions.\n",
    "- **Techniques**: Employ **walk-forward validation** and **rolling window validation** to assess model performance over time, accounting for dynamic market conditions.\n",
    "\n",
    "---\n",
    "\n",
    "### Advanced Optimization and Simulation Techniques (Planned)\n",
    "\n",
    "- **Objective**: Optimize model parameters using advanced methods like **Simulated Annealing** and **Genetic Algorithms** to improve predictive performance.\n",
    "- **Reasoning**: Current methods did not leverage optimization techniques for tuning parameters beyond grid search, and the non-linear nature of financial markets suggests that advanced optimization could significantly improve results.\n",
    "\n",
    "---\n",
    "\n",
    "### Monte Carlo Simulations for Risk and Uncertainty Assessment (Planned)\n",
    "\n",
    "- **Objective**: Incorporate **Monte Carlo simulations** to assess model uncertainty and stress-test predictions. This step will help quantify risk and improve decision-making strategies.\n",
    "- **Focus**: Simulations will be run on the final models to assess their robustness across varying market conditions, particularly for extreme events (e.g., market crashes or volatility spikes).\n",
    "\n",
    "---\n",
    "\n",
    "### Distribution Fitting and Simulation Methods (In Progress)\n",
    "\n",
    "- **Objective**: Enhance stock price simulation by using **Kernel Density Estimation** and other non-parametric methods to fit and simulate price distributions more accurately.\n",
    "- **Reasoning**: While EGARCH captured volatility well, there is a need to better simulate extreme price movements and outliers, which may be achieved through more flexible distribution-fitting techniques.\n",
    "\n",
    "---\n",
    "\n",
    "## Takeaways and Next Steps\n",
    "\n",
    "While the **EGARCH-enhanced models** did not universally improve performance across the board, they provided valuable insights into the complex volatility structures inherent in financial data. The **Ridge** and **LTS models** benefited from the volatility features, while **Elastic Net** and **VIF models** struggled to adapt. This mixed performance suggests that future work should focus on:\n",
    "\n",
    "1. **Exploring non-linear and machine learning approaches**: Methods like **Random Forests**, **GBM**, and **Neural Networks** can capture complex interactions and non-linearities better than linear regression models.\n",
    "   \n",
    "2. **Ensemble methods**: Combining the strengths of robust and machine learning models through stacking or blending will provide a more comprehensive predictive framework.\n",
    "   \n",
    "3. **Backtesting and validation**: Backtesting using rolling-window and walk-forward validation is crucial to ensuring robustness in live trading scenarios.\n",
    "\n",
    "4. **Risk quantification**: Implementing **Monte Carlo simulations** will enhance risk management and provide deeper insights into the range of possible outcomes under varying market conditions.\n",
    "\n",
    "The progress made so far has laid a strong foundation for further research into creating a highly effective quantitative trading strategy. By integrating these advanced methods and focusing on non-linearity, the next stage of this project will refine and enhance the overall predictive capability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
